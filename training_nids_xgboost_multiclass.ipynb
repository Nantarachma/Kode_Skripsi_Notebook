{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NIDS XGBoost Multi-Class ‚Äî Multi-Objective HPO\n",
    "\n",
    "> **Skripsi**: Network Intrusion Detection System (NIDS) menggunakan XGBoost  \n",
    "> **Dataset**: NF-UNSW-NB15-v3  \n",
    "> **Optimasi**: Multi-Objective Hyperparameter Optimization (TPE, NSGA-II, Random)  \n",
    "> **Objectives**: Maximize Macro F1-Score & Minimize Inference Latency\n",
    "\n",
    "Notebook ini berisi pipeline lengkap mulai dari preprocessing, optimasi hyperparameter,\n",
    "evaluasi model, hingga ekspor artefak untuk deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 1: Install & Import Libraries\n",
    "# ================================================================================\n",
    "# Instalasi library yang diperlukan (suppress output untuk cleaner notebook)\n",
    "!pip install -q xgboost optuna shap plotly kaleido\n",
    "\n",
    "# ================================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ================================================================================\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Data Processing ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Machine Learning Core ---\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    cohen_kappa_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# --- Optimization ---\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, NSGAIISampler, RandomSampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "\n",
    "# --- Interpretability ---\n",
    "import shap\n",
    "\n",
    "# --- Statistical Testing ---\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# ================================================================================\n",
    "# KONFIGURASI ENVIRONMENT\n",
    "# ================================================================================\n",
    "\n",
    "# Suppress warnings untuk output yang lebih bersih\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Pandas display options (FIXED:  hapus semua spasi di nama opsi!)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)  # ‚ùå SEBELUMNYA:  'display.  max_rows' (ada spasi!)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Matplotlib & Seaborn styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "\n",
    "# Plotly rendering (untuk Kaggle/Jupyter)\n",
    "pio.renderers.default = \"iframe\"  # Agar grafik interaktif muncul di Kaggle\n",
    "\n",
    "# Seed untuk reprodusibilitas\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ================================================================================\n",
    "# VERIFIKASI INSTALASI & GPU\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ SETUP ENVIRONMENT - NETWORK INTRUSION DETECTION SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python Version: {sys.version.split()[0]}\")\n",
    "print()\n",
    "\n",
    "# --- Verifikasi Library Versions ---\n",
    "print(\"üì¶ Library Versions:\")\n",
    "print(f\"   ‚îú‚îÄ NumPy:          {np.__version__}\")\n",
    "print(f\"   ‚îú‚îÄ Pandas:        {pd.__version__}\")\n",
    "print(f\"   ‚îú‚îÄ XGBoost:       {xgb.__version__} {'‚úÖ (v2.0+)' if int(xgb.__version__.split('.')[0]) >= 2 else '‚ö†Ô∏è (Perlu update! )'}\")\n",
    "print(f\"   ‚îú‚îÄ Optuna:         {optuna.__version__}\")\n",
    "print(f\"   ‚îú‚îÄ SHAP:          {shap.__version__}\")\n",
    "print(f\"   ‚îú‚îÄ Matplotlib:    {plt.matplotlib.__version__}\")\n",
    "print(f\"   ‚îî‚îÄ Seaborn:       {sns.__version__}\")\n",
    "print()\n",
    "\n",
    "# --- Verifikasi GPU/CUDA ---\n",
    "print(\"üñ•Ô∏è  Hardware & Compute:\")\n",
    "try:\n",
    "    # Check XGBoost GPU support\n",
    "    xgb_build_info = xgb.build_info()\n",
    "    use_cuda = xgb_build_info.get('USE_CUDA', False)\n",
    "    \n",
    "    if use_cuda:\n",
    "        print(f\"   ‚îú‚îÄ GPU Status:     ‚úÖ CUDA ENABLED\")\n",
    "        \n",
    "        # Try to get more GPU info (jika menggunakan environment dengan nvidia-smi)\n",
    "        try:\n",
    "            gpu_info = ! nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "            if gpu_info:\n",
    "                gpu_name, gpu_mem = gpu_info[0].split(', ')\n",
    "                print(f\"   ‚îú‚îÄ GPU Device:    {gpu_name}\")\n",
    "                print(f\"   ‚îî‚îÄ GPU Memory:    {gpu_mem}\")\n",
    "        except:\n",
    "            print(f\"   ‚îî‚îÄ GPU Device:    (Info tidak tersedia)\")\n",
    "    else:\n",
    "        print(f\"   ‚îî‚îÄ GPU Status:    ‚ö†Ô∏è  CUDA NOT AVAILABLE (akan menggunakan CPU)\")\n",
    "        print(f\"                     Tip: Aktifkan GPU di Kaggle Settings ‚Üí Accelerator\")\n",
    "except Exception as e: \n",
    "    print(f\"   ‚îî‚îÄ GPU Status:    ‚ö†Ô∏è  Error checking GPU: {str(e)}\")\n",
    "print()\n",
    "\n",
    "# --- Verifikasi Memory ---\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"üíæ System Memory:\")\n",
    "    print(f\"   ‚îú‚îÄ Total RAM:     {mem.total / (1024**3):.2f} GB\")\n",
    "    print(f\"   ‚îú‚îÄ Available:      {mem.available / (1024**3):.2f} GB\")\n",
    "    print(f\"   ‚îî‚îÄ Used:          {mem.percent:.1f}%\")\n",
    "except ImportError:\n",
    "    print(f\"üíæ System Memory:     (psutil tidak tersedia)\")\n",
    "print()\n",
    "\n",
    "# --- Konfigurasi Global untuk Penelitian ---\n",
    "print(\"‚öôÔ∏è  Research Configuration:\")\n",
    "print(f\"   ‚îú‚îÄ Random Seed:   {RANDOM_SEED}\")\n",
    "print(f\"   ‚îú‚îÄ Train/Test:     80/20 (stratified)\")\n",
    "print(f\"   ‚îú‚îÄ Validation:    20% from train (stratified)\")\n",
    "print(f\"   ‚îú‚îÄ Optuna Trials: 30 per sampler\")\n",
    "print(f\"   ‚îî‚îÄ Samplers:      TPE, NSGA-II, Random\")\n",
    "print()\n",
    "\n",
    "# --- Verifikasi XGBoost GPU Capability ---\n",
    "print(\"üî¨ XGBoost Configuration Test:\")\n",
    "try:\n",
    "    # Test apakah XGBoost bisa menggunakan GPU\n",
    "    test_params = {'tree_method': 'hist', 'device': 'cuda'}\n",
    "    test_model = xgb.XGBClassifier(**test_params, n_estimators=1, random_state=42)\n",
    "    \n",
    "    # Create dummy data\n",
    "    X_test = np.random.rand(100, 10)\n",
    "    y_test = np.random.randint(0, 2, 100)\n",
    "    \n",
    "    # Try to fit\n",
    "    test_model.fit(X_test, y_test, verbose=False)\n",
    "    print(f\"   ‚îî‚îÄ GPU Training:   ‚úÖ Successfully tested XGBoost GPU training\")\n",
    "except Exception as e: \n",
    "    print(f\"   ‚îî‚îÄ GPU Training:  ‚ö†Ô∏è  GPU test failed, akan fallback ke CPU\")\n",
    "    print(f\"                     Error: {str(e)}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ SETUP COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# HELPER FUNCTIONS (Opsional - untuk debugging)\n",
    "# ================================================================================\n",
    "\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Quick function to check GPU status\"\"\"\n",
    "    try:\n",
    "        build_info = xgb.build_info()\n",
    "        return build_info.get('USE_CUDA', False)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def print_dataset_info(df, name=\"Dataset\"):\n",
    "    \"\"\"Helper function to print dataset information\"\"\"\n",
    "    print(f\"\\nüìä {name} Information:\")\n",
    "    print(f\"   ‚îú‚îÄ Shape:         {df.shape[0]: ,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"   ‚îú‚îÄ Memory:         {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    print(f\"   ‚îú‚îÄ Duplicates:    {df.duplicated().sum():,}\")\n",
    "    print(f\"   ‚îî‚îÄ Missing:        {df.isnull().sum().sum():,} values\")\n",
    "\n",
    "# Simpan timestamp untuk tracking waktu eksekusi\n",
    "setup_time = datetime.now()\n",
    "print(f\"‚è±Ô∏è  Setup completed at: {setup_time.strftime('%H:%M:%S')}\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 2: Load Data, Class Mapping & Initial Split (UPDATED)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 1: AUTO-DETECT & LOAD DATASET\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìÇ LOADING DATASET NF-UNSW-NB15-v3\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüîç Mencari file dataset di /kaggle/input/ ...\")\n",
    "\n",
    "data_path = None\n",
    "\n",
    "# Auto-detect file dataset\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if (\n",
    "            filename.lower().endswith(\".csv\")\n",
    "            and \"unsw\" in filename.lower()\n",
    "            and \"features\" not in filename.lower()\n",
    "        ):\n",
    "            data_path = os.path.join(dirname, filename)\n",
    "            print(f\"‚úÖ Ditemukan file: {filename}\")\n",
    "            print(f\"   Path: {data_path}\")\n",
    "            break\n",
    "    if data_path:\n",
    "        break\n",
    "\n",
    "# Fallback manual path\n",
    "if not data_path:\n",
    "    print(\"‚ö†Ô∏è  Auto-detect gagal, menggunakan path manual...\")\n",
    "    data_path = '/kaggle/input/nf-unsw-nb15-v3/NF-UNSW-NB15-v3.csv'\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\n",
    "            \"‚ùå File data tidak ditemukan!\\n\"\n",
    "            \"   Pastikan dataset sudah di-add ke Kaggle Notebook Anda.\\n\"\n",
    "            f\"   Path yang dicoba: {data_path}\"\n",
    "        )\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\n‚è≥ Membaca dataset (ukuran besar, mohon tunggu)...\")\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "# Optimasi: Tentukan tipe data untuk hemat memori saat load jika memungkinkan\n",
    "# Namun load full dulu agar aman, baru di-cast nanti\n",
    "df_full = pd.read_csv(data_path)\n",
    "\n",
    "load_time = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "print(f\"‚úÖ Dataset berhasil dimuat dalam {load_time:.2f} detik\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 2: PEMETAAN KATEGORI (CLASS MAPPING) - SESUAI BAB 3\n",
    "# ================================================================================\n",
    "# Ini adalah bagian yang ditambahkan untuk mengubah label sesuai tabel Skripsi\n",
    "# 0: Normal, 1: DoS, 2: Probe, 3: Malware\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÑ TRANSFORMASI KATEGORI (MAPPING)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Identifikasi Kolom Kategori Asli\n",
    "# Di NF-UNSW-NB15-v3, kolom nama serangan biasanya 'Attack'\n",
    "if 'Attack' in df_full.columns:\n",
    "    source_col = 'Attack'\n",
    "elif 'attack_cat' in df_full.columns:\n",
    "    source_col = 'attack_cat'\n",
    "else:\n",
    "    # Fallback jika nama kolom berbeda, kita coba cari kolom object yang punya nilai 'Benign'\n",
    "    obj_cols = df_full.select_dtypes(include=['object']).columns\n",
    "    source_col = None\n",
    "    for col in obj_cols:\n",
    "        if 'Benign' in df_full[col].unique():\n",
    "            source_col = col\n",
    "            break\n",
    "            \n",
    "if not source_col:\n",
    "    raise ValueError(\"‚ùå Kolom kategori serangan (misal: 'Attack') tidak ditemukan!\")\n",
    "\n",
    "print(f\"‚úÖ Kolom sumber kategori ditemukan: '{source_col}'\")\n",
    "print(f\"‚ÑπÔ∏è  Jumlah unik awal: {df_full[source_col].nunique()} kategori\")\n",
    "\n",
    "# 2. Definisi Dictionary Mapping (Sesuai Tabel Anda)\n",
    "mapping_rules = {\n",
    "    # 0 (Normal)\n",
    "    'Benign': 0,\n",
    "    \n",
    "    # 1 (DoS)\n",
    "    'DoS': 1,\n",
    "    'Generic': 1,\n",
    "    \n",
    "    # 2 (Probe)\n",
    "    'Reconnaissance': 2,\n",
    "    'Analysis': 2,\n",
    "    \n",
    "    # 3 (Malware)\n",
    "    'Exploits': 3,\n",
    "    'Fuzzers': 3,\n",
    "    'Backdoor': 3,\n",
    "    'Shellcode': 3,\n",
    "    'Worms': 3\n",
    "}\n",
    "\n",
    "# 3. Terapkan Mapping\n",
    "print(\"\\nüõ†Ô∏è  Menerapkan mapping kategori...\")\n",
    "df_full['mapped_label'] = df_full[source_col].map(mapping_rules)\n",
    "\n",
    "# 4. Validasi Hasil Mapping\n",
    "unmapped_count = df_full['mapped_label'].isnull().sum()\n",
    "if unmapped_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  PERINGATAN: Ada {unmapped_count} sampel yang tidak ter-mapping!\")\n",
    "    print(\"   Label yang gagal dimapping:\", df_full[df_full['mapped_label'].isnull()][source_col].unique())\n",
    "    # Opsional: Drop atau masukkan ke kategori lain. Untuk saat ini kita drop agar aman.\n",
    "    df_full = df_full.dropna(subset=['mapped_label'])\n",
    "else:\n",
    "    print(\"‚úÖ Semua sampel berhasil di-mapping.\")\n",
    "\n",
    "# Ubah ke integer\n",
    "df_full['mapped_label'] = df_full['mapped_label'].astype(int)\n",
    "\n",
    "# 5. Set Target Column Baru\n",
    "target_col = 'mapped_label'\n",
    "\n",
    "# 6. Tampilkan Distribusi Baru\n",
    "print(\"\\nüìä DISTRIBUSI KELAS SETELAH MAPPING:\")\n",
    "mapping_names = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "dist_counts = df_full[target_col].value_counts().sort_index()\n",
    "\n",
    "print(f\"{'ID':<5} {'Kategori':<15} {'Jumlah':<15} {'Persentase':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for label_id, count in dist_counts.items():\n",
    "    label_name = mapping_names.get(label_id, 'Unknown')\n",
    "    pct = (count / len(df_full)) * 100\n",
    "    print(f\"{label_id:<5} {label_name:<15} {count:<15,} {pct:<10.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 3: ANALISIS DATASET (UPDATE)\n",
    "# ================================================================================\n",
    "# Kita skip analisis detail per kolom karena sudah ada di atas, fokus ke cek hasil mapping\n",
    "\n",
    "# Hapus kolom target lama agar tidak bocor (Data Leakage) saat training nanti\n",
    "cols_to_drop = ['Label', 'Attack', 'attack_cat'] \n",
    "# Hapus hanya jika ada di dataframe dan bukan target baru\n",
    "cols_to_drop = [c for c in cols_to_drop if c in df_full.columns and c != target_col]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nüóëÔ∏è  Menghapus kolom target lama (redundant): {cols_to_drop}\")\n",
    "    df_full.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 4: SPLIT TRAIN & TEST (80:20 STRATIFIED)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÇÔ∏è  PEMBAGIAN DATASET: TRAIN & TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ÑπÔ∏è  Melakukan split 80:20 dengan Stratified Sampling berdasarkan 'mapped_label'\")\n",
    "\n",
    "df_train_full, df_test_raw = train_test_split(\n",
    "    df_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_full[target_col] # Stratify menggunakan label baru\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split berhasil!\")\n",
    "print(f\"   - Train: {df_train_full.shape[0]:,} rows\")\n",
    "print(f\"   - Test : {df_test_raw.shape[0]:,} rows\")\n",
    "\n",
    "# Verifikasi Stratifikasi\n",
    "print(\"\\nüîç Verifikasi Proporsi Kelas di Data Uji (Test Set):\")\n",
    "test_dist = df_test_raw[target_col].value_counts(normalize=True).sort_index()\n",
    "for label_id, pct in test_dist.items():\n",
    "    label_name = mapping_names.get(label_id, 'Unknown')\n",
    "    print(f\"   - Kelas {label_id} ({label_name}): {pct*100:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 5: CLEANUP MEMORY\n",
    "# ================================================================================\n",
    "\n",
    "del df_full\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nüßπ Dataset original dihapus dari memori\")\n",
    "\n",
    "# ================================================================================\n",
    "# RINGKASAN FINAL CELL 2\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã RINGKASAN FINAL CELL 2\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Target Baru:     {target_col} (0=Normal, 1=DoS, 2=Probe, 3=Malware)\")\n",
    "print(f\"‚úÖ Train set shape: {df_train_full.shape}\")\n",
    "print(f\"‚úÖ Test set shape:  {df_test_raw.shape}\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing (Cleaning, Encoding, Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 3: Preprocessing (CLEANING, ENCODING & WEIGHTING) - FIXED\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "# --- 1. KONFIGURASI TARGET ---\n",
    "# Prioritaskan 'mapped_label' dari Cell 2, jika tidak ada baru cari yang lain\n",
    "possible_targets = ['mapped_label', 'Attack', 'attack_cat', 'Label'] \n",
    "target_col = next((col for col in df_train_full.columns if col in possible_targets), None)\n",
    "\n",
    "if not target_col:\n",
    "    raise ValueError(\"‚ùå Kolom target tidak ditemukan! Pastikan Cell 2 sudah dijalankan.\")\n",
    "\n",
    "print(f\"üéØ Target Kolom Final: '{target_col}'\")\n",
    "print(f\"üìä Shape dataset awal (train): {df_train_full.shape}\")\n",
    "print(f\"üìä Shape dataset awal (test):  {df_test_raw.shape}\")\n",
    "\n",
    "# --- 2. BERSIHKAN KOLOM IDENTITAS (WAJIB UTK NETFLOW) ---\n",
    "cols_to_drop = [\n",
    "    # 1-2. Metadata waktu (tidak konsisten untuk generalisasi)\n",
    "    'FLOW_START_MILLISECONDS',\n",
    "    'FLOW_END_MILLISECONDS',\n",
    "    \n",
    "    # 3-6. Identitas (risiko overfitting terhadap IP tertentu)\n",
    "    'IPV4_SRC_ADDR',\n",
    "    'IPV4_DST_ADDR',\n",
    "    \n",
    "    'Label'\n",
    "]\n",
    "\n",
    "# Verifikasi kolom ada sebelum drop (untuk TRAIN)\n",
    "cols_to_drop_safe_train = [col for col in cols_to_drop if col in df_train_full.columns and col != target_col]\n",
    "print(f\"\\nüóëÔ∏è Kolom yang akan di-drop (TRAIN): {len(cols_to_drop_safe_train)}\")\n",
    "# print(cols_to_drop_safe_train)\n",
    "\n",
    "# Verifikasi kolom ada sebelum drop (untuk TEST)\n",
    "cols_to_drop_safe_test = [col for col in cols_to_drop if col in df_test_raw.columns and col != target_col]\n",
    "# print(f\"üóëÔ∏è Kolom yang akan di-drop (TEST):  {len(cols_to_drop_safe_test)}\")\n",
    "\n",
    "# Drop kolom untuk TRAIN dan TEST\n",
    "df_train_clean = df_train_full.drop(columns=cols_to_drop_safe_train, errors='ignore')\n",
    "df_test_clean = df_test_raw.drop(columns=cols_to_drop_safe_test, errors='ignore')\n",
    "\n",
    "print(f\"\\n‚úÖ TRAIN - Kolom awal: {df_train_full.shape[1]} ‚Üí Setelah drop: {df_train_clean.shape[1]}\")\n",
    "print(f\"‚úÖ TEST  - Kolom awal: {df_test_raw.shape[1]} ‚Üí Setelah drop: {df_test_clean.shape[1]}\")\n",
    "\n",
    "# --- 3. PISAHKAN FITUR (X) & TARGET (y) ---\n",
    "X_raw = df_train_clean.drop(columns=[target_col], errors='ignore')\n",
    "y_raw = df_train_clean[target_col]\n",
    "\n",
    "X_test_final_raw = df_test_clean.drop(columns=[target_col], errors='ignore')\n",
    "y_test_final_raw = df_test_clean[target_col]\n",
    "\n",
    "print(f\"\\nüìã Fitur (X) shape: {X_raw.shape}\")\n",
    "print(f\"üìã Target (y) shape: {y_raw.shape}\")\n",
    "print(f\"üìã Test Fitur shape: {X_test_final_raw.shape}\")\n",
    "\n",
    "# --- 4. ENCODING FITUR KATEGORIKAL ---\n",
    "# Mendeteksi kolom object (string)\n",
    "cat_cols = X_raw.select_dtypes(include=['object']).columns\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    print(f\"\\nüî§ Encoding fitur kategorikal: {list(cat_cols)}\")\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Gabungkan train + test untuk fit encoder (agar tidak ada unseen categories)\n",
    "        full_col = pd.concat([X_raw[col], X_test_final_raw[col]], axis=0).astype(str)\n",
    "        le.fit(full_col)\n",
    "        \n",
    "        # Transform\n",
    "        X_raw[col] = le.transform(X_raw[col].astype(str))\n",
    "        X_test_final_raw[col] = le.transform(X_test_final_raw[col].astype(str))\n",
    "    print(f\"   ‚úÖ Encoding selesai untuk {len(cat_cols)} kolom kategorikal\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Tidak ada fitur kategorikal (semua numerik)\")\n",
    "\n",
    "# --- 5. PEMBERSIHAN DATA (CLEANING) ---\n",
    "print(\"\\nüßπ Membersihkan Infinity dan NaN values...\")\n",
    "\n",
    "# A. Ganti Infinity dengan NaN\n",
    "X_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test_final_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# B. Cek jumlah NaN sebelum cleaning\n",
    "nan_count_before = X_raw.isnull().sum().sum()\n",
    "print(f\"   üìå NaN sebelum cleaning: {nan_count_before:,}\")\n",
    "\n",
    "# C. Isi NaN dengan Median (Lebih aman daripada Mean untuk data jaringan yang miring)\n",
    "# Kita simpan median dari training data untuk dipakai juga di test data agar konsisten\n",
    "train_medians = X_raw.median()\n",
    "X_raw.fillna(train_medians, inplace=True)\n",
    "X_test_final_raw.fillna(train_medians, inplace=True)\n",
    "\n",
    "# D. Cek apakah masih ada error\n",
    "nan_count_after = X_raw.isnull().sum().sum()\n",
    "inf_count_after = np.isinf(X_raw).values.sum()\n",
    "\n",
    "print(f\"   üìå NaN setelah cleaning: {nan_count_after}\")\n",
    "print(f\"   üìå Inf setelah cleaning:  {inf_count_after}\")\n",
    "\n",
    "if inf_count_after > 0 or nan_count_after > 0:\n",
    "    print(\"   ‚ö†Ô∏è Masih ada nilai bermasalah! Melakukan pemaksaan 0.\")\n",
    "    X_raw.fillna(0, inplace=True)\n",
    "    X_test_final_raw.fillna(0, inplace=True)\n",
    "    print(\"   ‚úÖ Dipaksa menjadi 0\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Data 100% bersih dari Infinity dan NaN\")\n",
    "\n",
    "# --- 6. VALIDASI & ENCODING TARGET ---\n",
    "print(\"\\nüéØ Menyiapkan Target Label...\")\n",
    "# Pastikan target bertipe integer\n",
    "y = y_raw.astype(int)\n",
    "y_test_final = y_test_final_raw.astype(int)\n",
    "\n",
    "# Label Encoder tetap digunakan untuk memastikan urutan 0, 1, 2, ...\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(y)\n",
    "y_test_final = le_target.transform(y_test_final)\n",
    "\n",
    "print(f\"   üìå Jumlah kelas: {len(le_target.classes_)}\")\n",
    "print(f\"   üìå Daftar kelas: {le_target.classes_}\")\n",
    "\n",
    "# Mapping nama kelas (Jika dari Cell 2 sudah berupa angka, kita beri nama manual untuk display)\n",
    "# Sesuai Cell 2: 0=Normal, 1=DoS, 2=Probe, 3=Malware\n",
    "class_names_map = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "\n",
    "print(\"\\n   Mapping kelas (Numerik ‚Üí Nama):\")\n",
    "for i in le_target.classes_:\n",
    "    count = (y == i).sum()\n",
    "    pct = (count / len(y)) * 100\n",
    "    name = class_names_map.get(i, f\"Class {i}\")\n",
    "    print(f\"      {i} ‚Üí {name:15s}: {count:8,} ({pct:5.2f}%)\")\n",
    "\n",
    "# --- 7. SCALING (STANDARISASI) ---\n",
    "print(\"\\nüìè Melakukan Scaling data (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit pada train, transform pada train & test\n",
    "X_enc = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)\n",
    "X_test_enc = pd.DataFrame(scaler.transform(X_test_final_raw), columns=X_test_final_raw.columns)\n",
    "\n",
    "# Optimasi Memori (Convert float64 -> float32)\n",
    "float_cols = X_enc.select_dtypes(include=['float64']).columns\n",
    "if len(float_cols) > 0:\n",
    "    X_enc[float_cols] = X_enc[float_cols].astype('float32')\n",
    "    X_test_enc[float_cols] = X_test_enc[float_cols].astype('float32')\n",
    "    print(f\"   ‚úÖ {len(float_cols)} kolom dikonversi ke float32 (hemat memori)\")\n",
    "\n",
    "# Finalisasi Data\n",
    "X_selected = X_enc\n",
    "X_test_selected = X_test_enc\n",
    "    \n",
    "print(f\"\\n‚úÖ Dataset Siap Proses!\")\n",
    "print(f\"   üîπ Jumlah Fitur:  {X_selected.shape[1]}\")\n",
    "print(f\"   üîπ Train samples: {X_selected.shape[0]:,}\")\n",
    "print(f\"   üîπ Test samples:  {X_test_selected.shape[0]:,}\")\n",
    "\n",
    "# --- 8. SPLIT VALIDATION SET ---\n",
    "print(\"\\nüîÄ Membagi Train ‚Üí Train + Validation (80:20, stratified)...\")\n",
    "# X_train & y_train nanti dipakai untuk training model\n",
    "# X_val & y_val dipakai untuk evaluasi saat training (early stopping)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Train Set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   ‚úÖ Val Set:   {X_val.shape[0]:,} samples\")\n",
    "\n",
    "# --- 9. AUTO-WEIGHTING (HYBRID STRATEGY) ---\n",
    "print(\"\\n‚öñÔ∏è Menghitung Bobot Kelas (Hybrid: balanced ‚Üí sqrt ‚Üí normalize)...\")\n",
    "# Sesuai Bab 3.5.3: Penanganan Imbalance\n",
    "\n",
    "# A. Hitung bobot balanced standard\n",
    "raw_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# B. Transformasi sqrt (Untuk memperhalus penalti agar tidak terlalu ekstrem pada kelas minoritas)\n",
    "sample_weights_train = np.sqrt(raw_weights)\n",
    "\n",
    "# C. Normalisasi (Agar rata-rata bobot = 1, menjaga stabilitas learning rate)\n",
    "sample_weights_train = sample_weights_train / sample_weights_train.mean()\n",
    "\n",
    "print(f\"   ‚úÖ Bobot Selesai!\")\n",
    "print(f\"   üîπ Min Weight: {sample_weights_train.min():.4f}\")\n",
    "print(f\"   üîπ Max Weight: {sample_weights_train.max():.4f}\")\n",
    "print(f\"   üîπ Mean Weight: {sample_weights_train.mean():.4f}\")\n",
    "\n",
    "# Tampilkan statistik bobot per kelas\n",
    "print(\"\\n   Statistik bobot per kelas:\")\n",
    "for i in le_target.classes_:\n",
    "    mask = (y_train == i)\n",
    "    if mask.sum() > 0:\n",
    "        avg_weight = sample_weights_train[mask].mean()\n",
    "        name = class_names_map.get(i, str(i))\n",
    "        print(f\"      {name:15s}: avg weight = {avg_weight:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# RINGKASAN FINAL PREPROCESSING\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RINGKASAN PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Input Train:     {df_train_full.shape[0]:,} rows\")\n",
    "print(f\"‚úÖ Final Train:     {X_train.shape[0]:,} rows (untuk training)\")\n",
    "print(f\"‚úÖ Final Val:       {X_val.shape[0]:,} rows (untuk evaluasi)\")\n",
    "print(f\"‚úÖ Final Test:      {X_test_selected.shape[0]:,} rows (untuk testing akhir)\")\n",
    "print(f\"‚úÖ Jumlah Fitur:    {X_train.shape[1]}\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisasi Distribusi & Bobot Kelas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 4: Visualisasi Imbalance & Strategi Cost-Sensitive Learning (FINAL)\n",
    "# ================================================================================\n",
    "# ‚ö†Ô∏è JALANKAN CELL INI SETELAH CELL 3 (PREPROCESSING) SELESAI!\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä VISUALISASI DISTRIBUSI DATA & STRATEGI PENANGANAN IMBALANCE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 1: PERSIAPAN DATA & MAPPING LABEL\n",
    "# ================================================================================\n",
    "\n",
    "# 1. Definisi Mapping Nama Kelas (Agar grafik terbaca manusia, bukan angka)\n",
    "# Sesuai Cell 2: 0=Normal, 1=DoS, 2=Probe, 3=Malware\n",
    "label_map = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "\n",
    "# 2. Hitung jumlah sampel per kelas di Training Set\n",
    "class_counts_series = pd.Series(y_train).value_counts().sort_index()\n",
    "unique_classes = class_counts_series.index.tolist() # [0, 1, 2, 3]\n",
    "class_names_str = [label_map.get(i, str(i)) for i in unique_classes]\n",
    "\n",
    "print(\"üîπ Distribusi Kelas di Training Set:\")\n",
    "print(f\"   Total samples: {len(y_train):,}\")\n",
    "print(f\"   Total classes: {len(unique_classes)}\")\n",
    "print()\n",
    "\n",
    "# 3. Ambil bobot unik yang sudah dihitung di Cell 3\n",
    "# Kita ambil satu sampel bobot dari setiap kelas untuk representasi\n",
    "final_weights = []\n",
    "raw_weights_calc = compute_sample_weight('balanced', y=y_train) # Hitung ulang raw untuk perbandingan\n",
    "raw_weights_list = []\n",
    "\n",
    "for label_id in unique_classes:\n",
    "    # Cari indeks pertama yang memiliki label ini\n",
    "    idx = np.where(y_train == label_id)[0][0]\n",
    "    \n",
    "    # Ambil bobot final (Hybrid) dari array sample_weights_train\n",
    "    final_weights.append(sample_weights_train[idx])\n",
    "    \n",
    "    # Ambil bobot raw (Balanced standard)\n",
    "    raw_weights_list.append(raw_weights_calc[idx])\n",
    "\n",
    "# 4. Buat DataFrame Ringkasan\n",
    "stats_df = pd.DataFrame({\n",
    "    'ID_Kelas': unique_classes,\n",
    "    'Nama_Kelas': class_names_str,\n",
    "    'Jumlah_Sampel': class_counts_series.values,\n",
    "    'Persentase': (class_counts_series.values / len(y_train) * 100),\n",
    "    'Bobot_Raw': raw_weights_list,\n",
    "    'Bobot_Final_Hybrid': final_weights\n",
    "})\n",
    "\n",
    "# Hitung Effective Samples (Jumlah * Bobot)\n",
    "stats_df['Efektif_Sampel'] = stats_df['Jumlah_Sampel'] * stats_df['Bobot_Final_Hybrid']\n",
    "stats_df['Imbalance_Ratio'] = stats_df['Jumlah_Sampel'].max() / stats_df['Jumlah_Sampel']\n",
    "\n",
    "print(\"   Tabel Statistik Imbalance & Bobot:\")\n",
    "# Format tampilan agar rapi\n",
    "display_cols = ['Nama_Kelas', 'Jumlah_Sampel', 'Persentase', 'Bobot_Raw', 'Bobot_Final_Hybrid', 'Efektif_Sampel']\n",
    "print(stats_df[display_cols].to_string(index=False, float_format=\"%.4f\"))\n",
    "print()\n",
    "\n",
    "# Statistik Tambahan\n",
    "majority_class = stats_df.loc[stats_df['Jumlah_Sampel'].idxmax()]\n",
    "minority_class = stats_df.loc[stats_df['Jumlah_Sampel'].idxmin()]\n",
    "\n",
    "print(\"üîπ Highlight Statistik:\")\n",
    "print(f\"   - Kelas Mayoritas: {majority_class['Nama_Kelas']} ({majority_class['Jumlah_Sampel']:,} samples)\")\n",
    "print(f\"   - Kelas Minoritas: {minority_class['Nama_Kelas']} ({minority_class['Jumlah_Sampel']:,} samples)\")\n",
    "print(f\"   - Rasio Imbalance: {majority_class['Jumlah_Sampel'] / minority_class['Jumlah_Sampel']:.2f} : 1\")\n",
    "print(f\"   - Bobot Tertinggi: {stats_df['Bobot_Final_Hybrid'].max():.4f} (pada {minority_class['Nama_Kelas']})\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 2: VISUALISASI UTAMA (INPUT UNTUK GAMBAR SKRIPSI)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat visualisasi untuk Bab 3...\")\n",
    "sns.set_style(\"whitegrid\") # Style bersih untuk akademis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- PLOT A: Distribusi Jumlah Sampel (Log Scale) ---\n",
    "colors_dist = sns.color_palette(\"viridis\", len(unique_classes))\n",
    "bars1 = axes[0].bar(\n",
    "    stats_df['Nama_Kelas'],\n",
    "    stats_df['Jumlah_Sampel'],\n",
    "    color=colors_dist,\n",
    "    edgecolor='black',\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "axes[0].set_title(\"Distribusi Kelas (Skala Logaritmik)\", fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].set_yscale(\"log\") # PENTING: Log scale agar minoritas terlihat\n",
    "axes[0].set_ylabel(\"Jumlah Sampel (Log Scale)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Kategori Serangan\", fontsize=12)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.5, which='both')\n",
    "\n",
    "# Anotasi nilai di atas bar\n",
    "for bar, count, pct in zip(bars1, stats_df['Jumlah_Sampel'], stats_df['Persentase']):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(\n",
    "        bar.get_x() + bar.get_width()/2., \n",
    "        height * 1.15, \n",
    "        f'{count:,}\\n({pct:.1f}%)',\n",
    "        ha='center', va='bottom', fontsize=9, fontweight='bold', color='black'\n",
    "    )\n",
    "\n",
    "# --- PLOT B: Bobot Penyeimbang (Hybrid) ---\n",
    "colors_weight = sns.color_palette(\"magma_r\", len(unique_classes)) # Magma reverse agar bobot tinggi warnanya gelap\n",
    "bars2 = axes[1].bar(\n",
    "    stats_df['Nama_Kelas'],\n",
    "    stats_df['Bobot_Final_Hybrid'],\n",
    "    color=colors_weight,\n",
    "    edgecolor='black',\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "axes[1].set_title(\"Bobot Penyeimbang (Hybrid Strategy)\", fontsize=14, fontweight='bold', pad=15)\n",
    "axes[1].set_ylabel(\"Multiplier Bobot (Weight)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Kategori Serangan\", fontsize=12)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Garis referensi Mean=1.0\n",
    "axes[1].axhline(y=1.0, color='green', linestyle='--', linewidth=2, label='Baseline (1.0)')\n",
    "\n",
    "# Anotasi nilai bobot\n",
    "for bar, weight in zip(bars2, stats_df['Bobot_Final_Hybrid']):\n",
    "    axes[1].text(\n",
    "        bar.get_x() + bar.get_width()/2., \n",
    "        bar.get_height() + 0.1, \n",
    "        f'{weight:.2f}x',\n",
    "        ha='center', va='bottom', fontsize=10, fontweight='bold'\n",
    "    )\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribusi_dan_bobot_skripsi.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar 1 disimpan: 'distribusi_dan_bobot_skripsi.png'\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 3: VISUALISASI EFEK PEMBOBOTAN (BEFORE VS AFTER)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat visualisasi 'Effective Samples'...\")\n",
    "\n",
    "fig2, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(unique_classes))\n",
    "width = 0.35\n",
    "\n",
    "# Bar Original\n",
    "rects1 = ax.bar(x - width/2, stats_df['Jumlah_Sampel'], width, label='Sampel Asli', color='#3498db', edgecolor='black')\n",
    "# Bar Weighted\n",
    "rects2 = ax.bar(x + width/2, stats_df['Efektif_Sampel'], width, label='Sampel Efektif (Weighted)', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_title('Dampak Pembobotan terhadap \"Perhatian\" Model', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('Jumlah Sampel (Log Scale)', fontsize=12)\n",
    "ax.set_xlabel('Kategori Serangan', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stats_df['Nama_Kelas'], fontsize=11)\n",
    "ax.set_yscale('log') # Log scale lagi agar perbandingannya terlihat adil\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3, which='both')\n",
    "\n",
    "# Anotasi Kenaikan\n",
    "for i, (orig, eff) in enumerate(zip(stats_df['Jumlah_Sampel'], stats_df['Efektif_Sampel'])):\n",
    "    if eff > orig:\n",
    "        boost = eff / orig\n",
    "        ax.text(x[i] + width/2, eff * 1.2, f'Boost\\n{boost:.1f}x', ha='center', va='bottom', fontsize=9, color='#c0392b', fontweight='bold')\n",
    "    else:\n",
    "        shrink = orig / eff\n",
    "        ax.text(x[i] + width/2, eff * 1.2, f'Diredam\\n{shrink:.1f}x', ha='center', va='bottom', fontsize=9, color='#2980b9', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('impact_weighting_skripsi.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar 2 disimpan: 'impact_weighting_skripsi.png'\")\n",
    "\n",
    "# ================================================================================\n",
    "# BAGIAN 4: EXPORT DATA UNTUK LAMPIRAN\n",
    "# ================================================================================\n",
    "stats_df.to_csv('tabel_distribusi_bobot.csv', index=False)\n",
    "print(\"\\n‚úÖ Data tabel diekspor ke 'tabel_distribusi_bobot.csv' untuk lampiran skripsi.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Definisi Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 4: Objective Function Multi-Objective (Macro F1 vs Latency) - FIXED\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ DEFINISI OBJECTIVE FUNCTION UNTUK OPTUNA\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# KONFIGURASI GLOBAL\n",
    "# ================================================================================\n",
    "\n",
    "# Pastikan variabel ini ada dari cell sebelumnya\n",
    "NUM_CLASSES = len(le_target.classes_)\n",
    "CLASS_NAMES = le_target.classes_\n",
    "\n",
    "print(\"üîπ Konfigurasi Objective Function:\")\n",
    "print(f\"   - Target 1:            Maximize Macro F1-Score\")\n",
    "print(f\"   - Target 2:            Minimize Latency (¬µs/sample)\")\n",
    "print(f\"   - Jumlah kelas:        {NUM_CLASSES}\")\n",
    "print(f\"   - Device:              cuda (GPU)\")\n",
    "print(f\"   - Tree method:         hist\")\n",
    "print(f\"   - Sample weighting:    Enabled (Hybrid Strategy)\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# ================================================================================\n",
    "\n",
    "def objective_xgboost_multi(trial):\n",
    "    \"\"\"\n",
    "    Multi-objective function untuk Optuna. \n",
    "    \n",
    "    Returns:\n",
    "        tuple: (f1_macro, latency_us)\n",
    "            - f1_macro: Macro F1-Score (TO MAXIMIZE)\n",
    "            - latency_us: Waktu inferensi per sampel dalam mikrodetik (TO MINIMIZE)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # ========================================================================\n",
    "        # BAGIAN 1: HYPERPARAMETER SAMPLING\n",
    "        # ========================================================================\n",
    "        \n",
    "        param = {\n",
    "            # --- Booster Parameters ---\n",
    "            # Menggunakan range yang luas untuk eksplorasi\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            \n",
    "            # --- Tree Structure Parameters ---\n",
    "            'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "            'max_delta_step': trial.suggest_int('max_delta_step', 1, 8), # Penting utk imbalance\n",
    "            \n",
    "            # --- Regularization & Sampling ---\n",
    "            'gamma': trial.suggest_float('gamma', 0.1, 0.5),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-6, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-6, 1.0, log=True),\n",
    "            \n",
    "            # --- Fixed Parameters (GPU & Multiclass) ---\n",
    "            'objective': 'multi:softmax',   # Multiclass classification\n",
    "            'num_class': NUM_CLASSES,       # Jumlah kelas target\n",
    "            'tree_method': 'hist',          # Histogram-based (optimal untuk GPU)\n",
    "            'device': 'cuda',               # GPU acceleration\n",
    "            'eval_metric': 'mlogloss',      # Metric evaluasi internal\n",
    "            'verbosity': 0,                 # Suppress XGBoost warnings\n",
    "            'random_state': 42              # Reproducibility\n",
    "        }\n",
    "        \n",
    "        # ========================================================================\n",
    "        # BAGIAN 2: MODEL INITIALIZATION & TRAINING\n",
    "        # ========================================================================\n",
    "        \n",
    "        model = xgb.XGBClassifier(**param)\n",
    "        \n",
    "        # Training dengan sample weights\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights_train,  # Bobot dari Cell 3\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # ========================================================================\n",
    "        # BAGIAN 3: OBJECTIVE 2 - LATENCY (MINIMIZE)\n",
    "        # ========================================================================\n",
    "        # Kita ukur waktu dulu agar cache GPU 'hangat'\n",
    "        \n",
    "        # Warm-up (opsional, tapi baik untuk akurasi timer)\n",
    "        _ = model.predict(X_val.iloc[:100])\n",
    "        \n",
    "        # Pengukuran Waktu Presisi\n",
    "        start_time = time.perf_counter()\n",
    "        preds = model.predict(X_val) # Prediksi sekalian diambil untuk F1 score\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        total_time = end_time - start_time\n",
    "        # Konversi ke mikrodetik per sampel (¬µs/sample)\n",
    "        # 1 detik = 1,000,000 mikrodetik\n",
    "        latency_us = (total_time / len(X_val)) * 1_000_000\n",
    "        \n",
    "        # Validasi waktu\n",
    "        if latency_us <= 0:\n",
    "            raise ValueError(f\"Latency invalid: {latency_us}\")\n",
    "            \n",
    "        # ========================================================================\n",
    "        # BAGIAN 4: OBJECTIVE 1 - MACRO F1-SCORE (MAXIMIZE)\n",
    "        # ========================================================================\n",
    "        \n",
    "        f1_macro = f1_score(y_val, preds, average='macro')\n",
    "        \n",
    "        # Validasi score\n",
    "        if not (0 <= f1_macro <= 1):\n",
    "            raise ValueError(f\"F1 invalid: {f1_macro}\")\n",
    "            \n",
    "        # ========================================================================\n",
    "        # BAGIAN 5: LOGGING & USER ATTRS\n",
    "        # ========================================================================\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, preds)\n",
    "        \n",
    "        # Simpan atribut tambahan untuk analisis nanti\n",
    "        trial.set_user_attr('accuracy', accuracy)\n",
    "        trial.set_user_attr('total_time_sec', total_time)\n",
    "        \n",
    "        # Log progress sederhana\n",
    "        if trial.number % 10 == 0:\n",
    "            print(f\"   Trial {trial.number:3d}: F1={f1_macro:.4f}, Latency={latency_us:.2f}¬µs, Acc={accuracy:.4f}\")\n",
    "            \n",
    "        return f1_macro, latency_us\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Trial {trial.number} FAILED: {str(e)}\")\n",
    "        # Bersihkan memori jika fail\n",
    "        gc.collect()\n",
    "        # Return worst possible values: F1=0 (min), Latency=99999 (max)\n",
    "        return 0.0, 99999.0\n",
    "\n",
    "# ================================================================================\n",
    "# VALIDASI (DRY RUN)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üî¨ Melakukan validasi objective function (dry run)...\")\n",
    "\n",
    "# Dummy study\n",
    "dummy_study = optuna.create_study(\n",
    "    directions=['maximize', 'minimize'], \n",
    "    sampler=optuna.samplers.RandomSampler(seed=42)\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Jalankan 1 trial\n",
    "    dummy_study.optimize(objective_xgboost_multi, n_trials=1, show_progress_bar=False)\n",
    "    \n",
    "    trial_res = dummy_study.trials[0]\n",
    "    f1_test, lat_test = trial_res.values\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validasi SUKSES!\")\n",
    "    print(f\"   - F1-Score:       {f1_test:.4f}\")\n",
    "    print(f\"   - Latency:        {lat_test:.2f} ¬µs/sample\")\n",
    "    print(f\"   - Accuracy:       {trial_res.user_attrs.get('accuracy'):.4f}\")\n",
    "    \n",
    "    del dummy_study\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Validasi GAGAL: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ================================================================================\n",
    "# INFO SEARCH SPACE\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã PARAMETER SEARCH SPACE (SESUAI SKRIPSI & KODE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Parameter':<20} {'Tipe':<17} {'Range':<15} {'Fungsi'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Daftar parameter sesuai definisi di objective_xgboost_multi\n",
    "params_info = [\n",
    "    (\"n_estimators\",     \"int\",           \"[500, 2000]\",   \"Kompleksitas Model\"),\n",
    "    (\"learning_rate\",    \"float (log)\",   \"[0.01, 0.3]\",   \"Kecepatan Konvergensi\"),\n",
    "    (\"max_depth\",        \"int\",           \"[6, 12]\",       \"Kapasitas Menangkap Pola\"),\n",
    "    (\"min_child_weight\", \"int\",           \"[1, 7]\",        \"Bobot Min. Percabangan\"),\n",
    "    (\"max_delta_step\",   \"int\",           \"[1, 8]\",        \"Stabilisasi Imbalance\"),\n",
    "    (\"gamma\",            \"float\",         \"[0.1, 0.5]\",    \"Regularisasi Split\"),\n",
    "    (\"subsample\",        \"float\",         \"[0.6, 0.95]\",   \"Sampling Baris Data\"),\n",
    "    (\"colsample_bytree\", \"float\",         \"[0.5, 0.9]\",    \"Sampling Kolom Fitur\"),\n",
    "    (\"reg_alpha (L1)\",   \"float (log)\",   \"[1e-6, 1.0]\",   \"Feature Selection Soft\"),\n",
    "    (\"reg_lambda (L2)\",  \"float (log)\",   \"[1e-6, 1.0]\",   \"Mencegah Overfitting\")\n",
    "]\n",
    "\n",
    "for p_name, p_type, p_range, p_desc in params_info:\n",
    "    print(f\"{p_name:<20} {p_type:<17} {p_range:<15} {p_desc}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"‚úÖ Fungsi siap digunakan untuk optimasi NSGA-II, TPE, dan Random.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimasi Hyperparameter (TPE, NSGA-II, Random)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.0: Import & Konfigurasi Optimasi (UPGRADED)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "# Suppress experimental warnings dari Optuna\n",
    "warnings.filterwarnings('ignore', category=optuna.exceptions.ExperimentalWarning)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ OPTIMASI HYPERPARAMETER MULTI-OBJECTIVE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# KONFIGURASI EKSPERIMEN (SESUAIKAN DENGAN KUOTA GPU)\n",
    "# ================================================================================\n",
    "\n",
    "# Saran Skripsi: 50 trials agar Pareto Front terlihat jelas. \n",
    "# Jika GPU terbatas, kembalikan ke 30.\n",
    "N_TRIALS = 30\n",
    "SEED = 42\n",
    "\n",
    "# Definisi Arah Optimasi (PENTING AGAR KONSISTEN)\n",
    "# Objective 1: F1-Score -> Maximize\n",
    "# Objective 2: Latency  -> Minimize\n",
    "STUDY_DIRECTIONS = ['maximize', 'minimize']\n",
    "\n",
    "print(\"üîπ Konfigurasi Optimasi:\")\n",
    "print(f\"   - Jumlah trials per metode: {N_TRIALS}\")\n",
    "print(f\"   - Metode yang diuji:        NSGA-II, TPE, Random Sampler\")\n",
    "print(f\"   - Total evaluasi model:     {N_TRIALS * 3} kali training\")\n",
    "print(f\"   - Objectives:               1. Maximize F1-Macro\")\n",
    "print(f\"                               2. Minimize Inference Time\")\n",
    "print(f\"   - Random seed:              {SEED}\")\n",
    "\n",
    "# Estimasi Waktu (Kasar)\n",
    "print(\"\\n‚ö†Ô∏è  ESTIMASI WAKTU:\")\n",
    "print(\"   Jika 1 trial memakan waktu ¬±30-60 detik:\")\n",
    "print(f\"   Total waktu tunggu sekitar: {(N_TRIALS * 3 * 45) / 60:.0f} - {(N_TRIALS * 3 * 60) / 60:.0f} menit.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.1: Container Hasil Optimasi\n",
    "# ================================================================================\n",
    "\n",
    "optimization_results = {}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.2: TPE (Bayesian Optimization)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ [1/3] TPE (BAYESIAN OPTIMIZATION)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "start_tpe = time.time()\n",
    "\n",
    "study_tpe = optuna.create_study(\n",
    "    study_name=\"TPE_MultiObjective\",\n",
    "    directions=[\"maximize\", \"minimize\"],\n",
    "    sampler=TPESampler(seed=SEED)\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Menjalankan {N_TRIALS} trials...\")\n",
    "\n",
    "try:\n",
    "    study_tpe.optimize(\n",
    "        objective_xgboost_multi,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    time_tpe = time.time() - start_tpe\n",
    "\n",
    "    print(f\"\\n‚úÖ TPE selesai dalam {time_tpe:.2f} detik\")\n",
    "    print(f\"   - Trials completed:        {len(study_tpe.trials)}\")\n",
    "    print(f\"   - Pareto optimal trials:   {len(study_tpe.best_trials)}\")\n",
    "\n",
    "    optimization_results[\"TPE\"] = {\n",
    "        \"study\": study_tpe,\n",
    "        \"time\": time_tpe,\n",
    "        \"n_pareto\": len(study_tpe.best_trials)\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå TPE gagal: {e}\")\n",
    "    optimization_results[\"TPE\"] = {\"study\": None, \"time\": 0, \"n_pareto\": 0}\n",
    " "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.3: NSGA-II (Evolutionary Algorithm)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ [2/3] NSGA-II (EVOLUTIONARY ALGORITHM)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "start_nsga = time.time()\n",
    "\n",
    "study_nsga = optuna.create_study(\n",
    "    study_name=\"NSGAII_MultiObjective\",\n",
    "    directions=[\"maximize\", \"minimize\"],\n",
    "    sampler=NSGAIISampler(seed=SEED, population_size=10)\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Menjalankan {N_TRIALS} trials...\")\n",
    "\n",
    "try:\n",
    "    study_nsga.optimize(\n",
    "        objective_xgboost_multi,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    time_nsga = time.time() - start_nsga\n",
    "\n",
    "    print(f\"\\n‚úÖ NSGA-II selesai dalam {time_nsga:.2f} detik\")\n",
    "    print(f\"   - Trials completed:        {len(study_nsga.trials)}\")\n",
    "    print(f\"   - Pareto optimal trials:   {len(study_nsga.best_trials)}\")\n",
    "\n",
    "    optimization_results[\"NSGA-II\"] = {\n",
    "        \"study\": study_nsga,\n",
    "        \"time\": time_nsga,\n",
    "        \"n_pareto\": len(study_nsga.best_trials)\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå NSGA-II gagal: {e}\")\n",
    "    optimization_results[\"NSGA-II\"] = {\"study\": None, \"time\": 0, \"n_pareto\": 0}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.4: Random Sampler (Baseline)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ [3/3] RANDOM SAMPLER (BASELINE)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "start_random = time.time()\n",
    "\n",
    "study_random = optuna.create_study(\n",
    "    study_name=\"Random_MultiObjective\",\n",
    "    directions=[\"maximize\", \"minimize\"],\n",
    "    sampler=RandomSampler(seed=SEED)\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Menjalankan {N_TRIALS} trials...\")\n",
    "\n",
    "try:\n",
    "    study_random.optimize(\n",
    "        objective_xgboost_multi,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    time_random = time.time() - start_random\n",
    "\n",
    "    print(f\"\\n‚úÖ Random selesai dalam {time_random:.2f} detik\")\n",
    "    print(f\"   - Trials completed:        {len(study_random.trials)}\")\n",
    "    print(f\"   - Pareto optimal trials:   {len(study_random.best_trials)}\")\n",
    "\n",
    "    optimization_results[\"Random\"] = {\n",
    "        \"study\": study_random,\n",
    "        \"time\": time_random,\n",
    "        \"n_pareto\": len(study_random.best_trials)\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Random gagal: {e}\")\n",
    "    optimization_results[\"Random\"] = {\"study\": None, \"time\": 0, \"n_pareto\": 0}\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ekstraksi Parameter Pareto Optimal"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell Tambahan: Ekstraksi Parameter Pareto Optimal (ALL METHODS)\n",
    "# ================================================================================\n",
    "\n",
    "# 1. Konfigurasi Tampilan Pandas (Agar tabel tidak terpotong saat di-screenshot)\n",
    "pd.set_option('display.max_columns', None)        # Tampilkan semua kolom\n",
    "pd.set_option('display.max_rows', None)           # Tampilkan semua baris\n",
    "pd.set_option('display.width', 1000)              # Lebar display maksimal\n",
    "pd.set_option('display.float_format', '{:.4f}'.format) # Presisi angka desimal\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"üìä REKAPITULASI DETAIL PARAMETER SOLUSI PARETO OPTIMAL (SEMUA METODE)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Fungsi bantuan untuk ekstrak data\n",
    "def extract_pareto_data(study, method_name):\n",
    "    if study is None:\n",
    "        return None\n",
    "    \n",
    "    best_trials = study.best_trials\n",
    "    data = []\n",
    "    for trial in best_trials:\n",
    "        row = {\n",
    "            \"Trial ID\": trial.number,\n",
    "            \"Macro F1\": trial.values[0],\n",
    "            \"Latensi (¬µs)\": trial.values[1],\n",
    "            **trial.params # Unpack semua parameter (learning_rate, max_depth, dll)\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Urutkan berdasarkan F1 tertinggi\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=\"Macro F1\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ================================================================================\n",
    "# 1. METODE TPE (Tree-structured Parzen Estimator)\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "print(\"üèÜ 1. METODE TPE (Bayesian)\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "\n",
    "if \"TPE\" in optimization_results and optimization_results[\"TPE\"][\"study\"] is not None:\n",
    "    df_tpe = extract_pareto_data(optimization_results[\"TPE\"][\"study\"], \"TPE\")\n",
    "    print(f\"‚úÖ Ditemukan {len(df_tpe)} solusi Pareto.\")\n",
    "    print(\"\\nüìã TABEL KONFIGURASI TPE :\")\n",
    "    print(df_tpe)\n",
    "    # Simpan\n",
    "    df_tpe.to_csv(\"pareto_params_tpe.csv\", index=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data TPE tidak ditemukan.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. METODE NSGA-II (Evolutionary)\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "print(\"üß¨ 2. METODE NSGA-II (Evolutionary)\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "\n",
    "if \"NSGA-II\" in optimization_results and optimization_results[\"NSGA-II\"][\"study\"] is not None:\n",
    "    df_nsga = extract_pareto_data(optimization_results[\"NSGA-II\"][\"study\"], \"NSGA-II\")\n",
    "    print(f\"‚úÖ Ditemukan {len(df_nsga)} solusi Pareto.\")\n",
    "    print(\"\\nüìã TABEL KONFIGURASI NSGA-II :\")\n",
    "    print(df_nsga)\n",
    "    # Simpan\n",
    "    df_nsga.to_csv(\"pareto_params_nsgaii.csv\", index=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data NSGA-II tidak ditemukan.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. METODE RANDOM SEARCH (Baseline)\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "print(\"üé≤ 3. METODE RANDOM SEARCH (Baseline)\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "\n",
    "# Cek nama key (kadang 'Random' kadang 'RandomSearch')\n",
    "key_random = \"Random\" if \"Random\" in optimization_results else \"RandomSearch\"\n",
    "\n",
    "if key_random in optimization_results and optimization_results[key_random][\"study\"] is not None:\n",
    "    df_random = extract_pareto_data(optimization_results[key_random][\"study\"], \"Random\")\n",
    "    print(f\"‚úÖ Ditemukan {len(df_random)} solusi non-dominated.\")\n",
    "    print(\"\\nüìã TABEL KONFIGURASI RANDOM SEARCH :\")\n",
    "    print(df_random)\n",
    "    # Simpan\n",
    "    df_random.to_csv(\"pareto_params_random.csv\", index=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data Random Search tidak ditemukan.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úÖ SELESAI. File CSV juga telah disimpan untuk lampiran.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 5.5: Ringkasan Perbandingan Metode\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä RINGKASAN PERBANDINGAN METODE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for method, result in optimization_results.items():\n",
    "    if result[\"study\"] is not None and result[\"study\"].best_trials:\n",
    "        best_trial = max(result[\"study\"].best_trials, key=lambda t: t.values[0])\n",
    "        best_f1, best_time = best_trial.values\n",
    "    else:\n",
    "        best_f1, best_time = 0.0, 9999.0\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Metode\": method,\n",
    "        \"Waktu_Optimasi_s\": result[\"time\"],\n",
    "        \"Waktu_Optimasi_min\": result[\"time\"] / 60,\n",
    "        \"Pareto_Solutions\": result[\"n_pareto\"],\n",
    "        \"Best_F1\": best_f1,\n",
    "        \"Best_InferenceTime\": best_time\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluasi Model Final"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 8: Evaluasi Final (TPE vs NSGA-II vs Random) - FIXED\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üèÜ EVALUASI FINAL - PERBANDINGAN 3 METODE OPTIMASI\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Mapping Nama Kelas (Agar Output Rapi)\n",
    "# Pastikan label_map sudah ada, kalau belum kita definisikan ulang\n",
    "label_map = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "target_names = [label_map.get(i, str(i)) for i in range(len(label_map))]\n",
    "\n",
    "# ================================================================================\n",
    "# 1. HELPER: PILIH SOLUSI TERBAIK DARI PARETO FRONT\n",
    "# ================================================================================\n",
    "\n",
    "def get_best_params_strategy(study, name):\n",
    "    \"\"\"\n",
    "    Strategi pemilihan model terbaik:\n",
    "    - Untuk Single Objective (Random/TPE): Ambil best_trial biasa.\n",
    "    - Untuk Multi Objective (NSGA-II): Ambil solusi dengan F1-Score Tertinggi \n",
    "      (Prioritas Keamanan) dari Pareto Front.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Menganalisis hasil optimasi {name}...\")\n",
    "    \n",
    "    # Cek apakah study valid\n",
    "    if len(study.trials) == 0:\n",
    "        raise ValueError(f\"Study {name} kosong!\")\n",
    "\n",
    "    # Logika Pengambilan Best Trial\n",
    "    if name == \"NSGA-II\":\n",
    "        # Multi-objective: study.best_trials mengembalikan list Pareto Front\n",
    "        pareto_front = study.best_trials\n",
    "        if not pareto_front:\n",
    "            # Fallback jika pareto kosong (jarang terjadi)\n",
    "            best_trial = study.trials[0]\n",
    "        else:\n",
    "            # STRATEGI SKRIPSI: Pilih solusi dengan F1 tertinggi (Objective 0)\n",
    "            # Trade-off: Kita korbankan sedikit waktu demi akurasi maksimal untuk perbandingan ini\n",
    "            best_trial = max(pareto_front, key=lambda t: t.values[0])\n",
    "            \n",
    "        print(f\"   ‚ÑπÔ∏è  NSGA-II: Dipilih solusi Pareto dengan F1 tertinggi dari {len(pareto_front)} kandidat.\")\n",
    "    else:\n",
    "        # Single objective (atau jika TPE dijadikan multi, logicnya mirip NSGA-II)\n",
    "        # Jika TPE di-set multi-obj di Cell 5, gunakan logika pareto juga\n",
    "        if len(study.directions) > 1:\n",
    "             pareto_front = study.best_trials\n",
    "             best_trial = max(pareto_front, key=lambda t: t.values[0])\n",
    "        else:\n",
    "             best_trial = study.best_trial\n",
    "\n",
    "    # Ambil Values\n",
    "    val_f1 = best_trial.values[0]\n",
    "    val_lat = best_trial.values[1]\n",
    "\n",
    "    print(f\"   ‚úÖ Best Trial ID: #{best_trial.number}\")\n",
    "    print(f\"      - Validasi F1:      {val_f1:.4f}\")\n",
    "    print(f\"      - Validasi Latency: {val_lat:.2f} ¬µs\")\n",
    "\n",
    "    return best_trial.params\n",
    "\n",
    "# ================================================================================\n",
    "# 2. FUNGSI TRAINING ULANG (RETRAIN)\n",
    "# ================================================================================\n",
    "\n",
    "def train_and_evaluate(study, name):\n",
    "    # 1. Ambil Parameter Terbaik\n",
    "    best_params = get_best_params_strategy(study, name)\n",
    "    \n",
    "    # 2. Tambahkan Parameter Tetap (Fixed)\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(target_names),\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42\n",
    "    })\n",
    "    \n",
    "    # 3. Training Ulang\n",
    "    # Catatan Metodologi: Gunakan X_train dan X_val terpisah untuk mencegah Data Leakage.\n",
    "    # Jangan digabung (X_full) jika menggunakan eval_set.\n",
    "    \n",
    "    print(f\"   ‚è≥ Training ulang model {name} ({best_params['n_estimators']} trees)...\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    \n",
    "    start_train = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights_train, # Bobot Hybrid dari Cell 3\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False # Supress log biar notebook bersih\n",
    "    )\n",
    "    train_dur = time.time() - start_train\n",
    "    print(f\"   ‚úÖ Training selesai dalam {train_dur:.2f} detik.\")\n",
    "\n",
    "    # 4. Evaluasi pada TEST SET (Unseen Data)\n",
    "    print(\"   ‚è±Ô∏è  Mengukur performa di Test Set...\")\n",
    "    \n",
    "    # Warm-up GPU\n",
    "    _ = model.predict(X_test_selected.iloc[:100])\n",
    "    \n",
    "    # Ukur Waktu Inferensi Presisi\n",
    "    start_inf = time.perf_counter()\n",
    "    preds = model.predict(X_test_selected)\n",
    "    end_inf = time.perf_counter()\n",
    "    \n",
    "    # Hitung Latency (Microseconds per Sample)\n",
    "    total_inf_time = end_inf - start_inf\n",
    "    latency_us = (total_inf_time / len(X_test_selected)) * 1_000_000\n",
    "    \n",
    "    return model, preds, latency_us, train_dur\n",
    "\n",
    "# ================================================================================\n",
    "# 3. EKSEKUSI LOOP EVALUASI\n",
    "# ================================================================================\n",
    "\n",
    "results_data = []\n",
    "trained_models = {}\n",
    "\n",
    "# List study yang akan dievaluasi (Pastikan variabel study_tpe dll ada dari Cell 7)\n",
    "studies_map = []\n",
    "if 'study_nsga' in locals(): studies_map.append((\"NSGA-II\", study_nsga))\n",
    "if 'study_tpe' in locals():  studies_map.append((\"TPE\", study_tpe))\n",
    "if 'study_random' in locals(): studies_map.append((\"Random\", study_random))\n",
    "\n",
    "if not studies_map:\n",
    "    print(\"‚ùå TIDAK ADA STUDY YANG DITEMUKAN. Jalankan Cell 5-7 dulu!\")\n",
    "else:\n",
    "    for name, study in studies_map:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(f\"üî¨ PROSES: {name}\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Jalankan Training & Evaluasi\n",
    "            model, preds, latency, train_time = train_and_evaluate(study, name)\n",
    "            \n",
    "            # Simpan Model Global\n",
    "            trained_models[name] = model\n",
    "            \n",
    "            # Hitung Metrik Akurasi Final\n",
    "            acc = accuracy_score(y_test_final, preds)\n",
    "            f1 = f1_score(y_test_final, preds, average='macro')\n",
    "            \n",
    "            # Simpan Hasil\n",
    "            results_data.append({\n",
    "                'Metode': name,\n",
    "                'F1_Macro': f1,\n",
    "                'Accuracy': acc,\n",
    "                'Latency (¬µs)': latency,\n",
    "                'Training Time (s)': train_time,\n",
    "                'Best Params': str(model.get_params()['n_estimators']) + \" trees\" # Contoh ringkasan\n",
    "            })\n",
    "            \n",
    "            # Cetak Laporan Singkat\n",
    "            print(f\"\\nüìÑ Laporan Klasifikasi ({name}):\")\n",
    "            print(classification_report(y_test_final, preds, target_names=target_names, digits=4))\n",
    "            \n",
    "            # Opsional: Confusion Matrix text\n",
    "            # cm = confusion_matrix(y_test_final, preds)\n",
    "            # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Gagal mengevaluasi {name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# ================================================================================\n",
    "# 4. REKAPITULASI HASIL (OUTPUT TABEL SKRIPSI)\n",
    "# ================================================================================\n",
    "\n",
    "if results_data:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ REKAPITULASI HASIL AKHIR (TEST SET)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    df_results = df_results.set_index('Metode')\n",
    "    \n",
    "    # Format tampilan\n",
    "    print(df_results[['F1_Macro', 'Accuracy', 'Latency (¬µs)', 'Training Time (s)']].to_string(float_format=\"%.4f\"))\n",
    "    \n",
    "    # Cari Pemenang\n",
    "    best_f1_method = df_results['F1_Macro'].idxmax()\n",
    "    best_lat_method = df_results['Latency (¬µs)'].idxmin()\n",
    "    \n",
    "    print(\"\\n‚úÖ KESIMPULAN:\")\n",
    "    print(f\"   - Juara Akurasi (F1):  {best_f1_method} ({df_results.loc[best_f1_method, 'F1_Macro']:.4f})\")\n",
    "    print(f\"   - Juara Kecepatan:     {best_lat_method} ({df_results.loc[best_lat_method, 'Latency (¬µs)']:.2f} ¬µs)\")\n",
    "    \n",
    "    # Simpan CSV\n",
    "    df_results.to_csv('tabel_perbandingan_metode.csv')\n",
    "    print(\"\\nüíæ Tabel disimpan ke 'tabel_perbandingan_metode.csv'\")\n",
    "    \n",
    "    # Clean memory GPU\n",
    "    del model\n",
    "    gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisasi Hasil\n",
    "### 9A. Pareto Front Gabungan"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 9: Visualisasi Pareto Front (Static / Publication Ready) - Matplotlib\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "# Set style agar terlihat profesional (seperti jurnal)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä VISUALISASI PARETO FRONT (STATIC) - PERBANDINGAN METODE OPTIMASI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================================\n",
    "# 1. KONFIGURASI DATA & VALIDASI\n",
    "# ================================================================================\n",
    "\n",
    "# Validasi X_val untuk konversi waktu\n",
    "try:\n",
    "    VAL_SIZE = len(X_val)\n",
    "    print(f\"‚ÑπÔ∏è  Ukuran Validation Set: {VAL_SIZE:,} sampel (untuk kalkulasi Latency)\")\n",
    "except NameError:\n",
    "    # Fallback dummy jika X_val tidak ada (hanya agar kode jalan)\n",
    "    VAL_SIZE = 1000 \n",
    "    print(\"‚ö†Ô∏è X_val tidak ditemukan. Menggunakan nilai dummy 1000.\")\n",
    "\n",
    "available_studies = {}\n",
    "# Cek ketersediaan studi (pastikan nama variabel sesuai dengan cell sebelumnya)\n",
    "if 'study_tpe' in locals() and study_tpe: available_studies['TPE (Single)'] = study_tpe\n",
    "if 'study_nsga' in locals() and study_nsga: available_studies['NSGA-II (Multi)'] = study_nsga\n",
    "if 'study_random' in locals() and study_random: available_studies['Random Search'] = study_random\n",
    "\n",
    "if not available_studies:\n",
    "    raise ValueError(\"‚ùå Tidak ada study yang tersedia! Jalankan Cell 5-7 dulu.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. HELPER: EKSTRAKSI DATA\n",
    "# ================================================================================\n",
    "\n",
    "def get_study_data_static(study, name):\n",
    "    data_points = []\n",
    "    \n",
    "    # Ambil trial yang sukses\n",
    "    valid_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    \n",
    "    # Ambil ID dari Best Trials (Pareto Front)\n",
    "    # Kita pakai set ID karena object trial tidak bisa di-hash\n",
    "    pareto_ids = set([t.number for t in study.best_trials])\n",
    "    \n",
    "    for t in valid_trials:\n",
    "        # Asumsi: values[0] = F1 (Maximize), values[1] = Time (Minimize)\n",
    "        f1 = t.values[0]\n",
    "        raw_time = t.values[1]\n",
    "        \n",
    "        # Konversi ke Microseconds per sample\n",
    "        latency_us = (raw_time / VAL_SIZE) * 1_000_000\n",
    "        \n",
    "        is_pareto = t.number in pareto_ids\n",
    "        \n",
    "        data_points.append({\n",
    "            'Method': name,\n",
    "            'F1_Score': f1,\n",
    "            'Latency_us': latency_us,\n",
    "            'Is_Pareto': is_pareto\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data_points)\n",
    "\n",
    "# ================================================================================\n",
    "# 3. PLOTTING (MATPLOTLIB)\n",
    "# ================================================================================\n",
    "\n",
    "# Buat Canvas Gambar (Ukuran HD)\n",
    "plt.figure(figsize=(10, 7), dpi=150) # DPI tinggi agar teks tajam\n",
    "\n",
    "# Definisi Warna & Marker\n",
    "colors = {\n",
    "    'NSGA-II (Multi)': '#d62728',  # Merah (Highlight)\n",
    "    'TPE (Single)':    '#1f77b4',  # Biru\n",
    "    'Random Search':   '#7f7f7f'   # Abu-abu\n",
    "}\n",
    "markers = {\n",
    "    'NSGA-II (Multi)': 'o',  # Bulat\n",
    "    'TPE (Single)':    'D',  # Diamond\n",
    "    'Random Search':   's'   # Kotak\n",
    "}\n",
    "\n",
    "# Loop setiap metode untuk plotting\n",
    "for name, study in available_studies.items():\n",
    "    df = get_study_data_static(study, name)\n",
    "    c = colors.get(name, 'black')\n",
    "    m = markers.get(name, 'o')\n",
    "    \n",
    "    # 1. Plot Titik Eksplorasi (Titik kecil transparan)\n",
    "    non_pareto = df[~df['Is_Pareto']]\n",
    "    plt.scatter(\n",
    "        non_pareto['Latency_us'], \n",
    "        non_pareto['F1_Score'], \n",
    "        color=c, alpha=0.2, s=20, label=None # Label none agar tidak menuhin legenda\n",
    "    )\n",
    "    \n",
    "    # 2. Plot Pareto Front (Titik besar solid + Garis)\n",
    "    pareto = df[df['Is_Pareto']].sort_values(by='Latency_us')\n",
    "    \n",
    "    if len(pareto) > 0:\n",
    "        # Gambar Garis Penghubung Pareto\n",
    "        plt.plot(\n",
    "            pareto['Latency_us'], \n",
    "            pareto['F1_Score'], \n",
    "            color=c, linestyle='--', linewidth=1.5, alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Gambar Titik Pareto\n",
    "        plt.scatter(\n",
    "            pareto['Latency_us'], \n",
    "            pareto['F1_Score'], \n",
    "            color=c, marker=m, s=80, edgecolors='white', linewidth=1.5,\n",
    "            label=f\"{name} (Frontier)\", zorder=5\n",
    "        )\n",
    "\n",
    "# ================================================================================\n",
    "# 4. KOSMETIK & PENYIMPANAN\n",
    "# ================================================================================\n",
    "\n",
    "plt.title('Multi-Objective Optimization: Accuracy vs Latency', fontsize=14, pad=15, fontweight='bold')\n",
    "plt.xlabel('Inference Latency (¬µs/sample) $\\\\rightarrow$ Lower is Better', fontsize=11)\n",
    "plt.ylabel('Macro F1-Score $\\\\rightarrow$ Higher is Better', fontsize=11)\n",
    "\n",
    "# Tambahkan Legend\n",
    "plt.legend(title='Optimization Method', loc='lower right', frameon=True, framealpha=0.9)\n",
    "\n",
    "# Grid & Layout\n",
    "plt.grid(True, linestyle=':', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Simpan Gambar\n",
    "filename = 'pareto_front_static_hd.png'\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight') # 300 DPI = Kualitas Cetak\n",
    "print(f\"\\n‚úÖ Gambar berhasil disimpan sebagai: {filename}\")\n",
    "\n",
    "# Tampilkan di Notebook\n",
    "plt.show()\n",
    "\n",
    "# ================================================================================\n",
    "# 5. INTERPRETASI\n",
    "# ================================================================================\n",
    "print(\"\\nüìã CARA MEMBACA GRAFIK INI:\")\n",
    "print(\"1. Titik-titik transparan kecil adalah percobaan yang gagal/kurang optimal.\")\n",
    "print(\"2. Titik-titik BESAR yang terhubung garis putus-putus adalah SOLUSI TERBAIK (Pareto Front).\")\n",
    "print(\"3. Metode terbaik adalah yang garisnya paling mendekati pojok KIRI ATAS.\")\n",
    "print(\"   (Artinya: Latency rendah/kiri DAN F1-Score tinggi/atas).\")\n",
    "print(\"4. Bandingkan garis Merah (NSGA-II) vs Biru (TPE).\")\n",
    "print(\"   - Jika Merah lebih ke kiri-atas dari Biru -> NSGA-II menang.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9B. Confusion Matrix (Raw & Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 10A: Visualisasi Confusion Matrix (Raw & Normalized Heatmaps)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä BAGIAN A: VISUALISASI CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(\"Tujuan: Memvisualisasikan sebaran prediksi vs label asli.\")\n",
    "print()\n",
    "\n",
    "# 1. Validasi Input\n",
    "if 'trained_models' not in locals() or len(trained_models) == 0:\n",
    "    raise ValueError(\"‚ùå 'trained_models' tidak ditemukan! Pastikan Cell 8 sudah dijalankan.\")\n",
    "\n",
    "label_map = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "class_names = [label_map.get(i, str(i)) for i in range(len(label_map))]\n",
    "methods = list(trained_models.keys()) # TPE, Random, NSGA-II\n",
    "\n",
    "# 2. Helper Plotting\n",
    "def plot_heatmap(y_true, y_pred, title, ax, fmt, cmap):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalisasi jika format persentase\n",
    "    if '%' in fmt:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm = np.nan_to_num(cm)\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=fmt, cmap=cmap, cbar=False, ax=ax,\n",
    "        xticklabels=class_names, yticklabels=class_names,\n",
    "        linewidths=0.5, linecolor='grey',\n",
    "        annot_kws={\"size\": 10, \"weight\": \"bold\"}\n",
    "    )\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold', pad=10)\n",
    "    ax.set_xlabel('Prediksi', fontsize=9)\n",
    "    ax.set_ylabel('Asli', fontsize=9)\n",
    "\n",
    "# 3. Plotting Raw Counts\n",
    "print(\"üì∏ Generating Raw Confusion Matrix (Jumlah Sampel)...\")\n",
    "fig1, axes1 = plt.subplots(1, len(methods), figsize=(6 * len(methods), 5))\n",
    "if len(methods) == 1: axes1 = [axes1]\n",
    "\n",
    "for i, name in enumerate(methods):\n",
    "    model = trained_models[name]\n",
    "    preds = model.predict(X_test_selected)\n",
    "    plot_heatmap(y_test_final, preds, f\"{name} (Raw Counts)\", axes1[i], 'd', 'Blues')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cm_raw_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. Plotting Normalized (%)\n",
    "print(\"\\nüì∏ Generating Normalized Confusion Matrix (Sensitivitas/Recall)...\")\n",
    "fig2, axes2 = plt.subplots(1, len(methods), figsize=(6 * len(methods), 5))\n",
    "if len(methods) == 1: axes2 = [axes2]\n",
    "\n",
    "for i, name in enumerate(methods):\n",
    "    model = trained_models[name]\n",
    "    preds = model.predict(X_test_selected)\n",
    "    plot_heatmap(y_test_final, preds, f\"{name} (Normalized %)\", axes2[i], '.1%', 'RdYlGn')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cm_norm_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ CELL 10A SELESAI. Grafik disimpan: 'cm_raw_heatmap.png' & 'cm_norm_heatmap.png'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9C. Analisis Statistik (Cohen's Kappa & Error Breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 10B: Analisis Statistik (Cohen's Kappa & Error Breakdown)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù BAGIAN B: ANALISIS STATISTIK & POLA KESALAHAN\")\n",
    "print(\"=\"*80)\n",
    "print(\"Tujuan: Mengukur reliabilitas (Kappa) dan mendeteksi kesalahan spesifik.\")\n",
    "print()\n",
    "\n",
    "# 1. Hitung Cohen's Kappa\n",
    "kappa_data = []\n",
    "\n",
    "print(\"üîπ Menghitung Reliabilitas Model (Cohen's Kappa)...\")\n",
    "for name, model in trained_models.items():\n",
    "    preds = model.predict(X_test_selected)\n",
    "    score = cohen_kappa_score(y_test_final, preds)\n",
    "    \n",
    "    interpretasi = \"Moderate\"\n",
    "    if score > 0.8: interpretasi = \"Almost Perfect (Sangat Andal)\"\n",
    "    elif score > 0.6: interpretasi = \"Substantial (Andal)\"\n",
    "    \n",
    "    kappa_data.append({'Metode': name, 'Kappa Score': score, 'Interpretasi': interpretasi})\n",
    "\n",
    "# Tampilkan Tabel\n",
    "kappa_df = pd.DataFrame(kappa_data).sort_values('Kappa Score', ascending=False)\n",
    "print(kappa_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# 2. Visualisasi Kappa Comparison\n",
    "plt.figure(figsize=(8, 4))\n",
    "colors = ['#2ecc71' if x > 0.8 else '#f1c40f' for x in kappa_df['Kappa Score']]\n",
    "plt.barh(kappa_df['Metode'], kappa_df['Kappa Score'], color=colors, edgecolor='black', height=0.6)\n",
    "plt.title(\"Perbandingan Reliabilitas Model (Cohen's Kappa)\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Score (0-1)\", fontsize=10)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.axvline(0.8, color='red', linestyle='--', alpha=0.5, label='Threshold Excellent (0.8)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('kappa_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. Analisis Pola Kesalahan (Deep Dive)\n",
    "print(\"\\nüîπ Bedah Kesalahan (Top 3 Misclassification per Metode):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name in trained_models.keys():\n",
    "    print(f\"üìå Analisis Metode: {name}\")\n",
    "    model = trained_models[name]\n",
    "    preds = model.predict(X_test_selected)\n",
    "    cm = confusion_matrix(y_test_final, preds)\n",
    "    \n",
    "    # Cari kesalahan (Off-diagonal)\n",
    "    errors = []\n",
    "    for r in range(len(class_names)): # Asli\n",
    "        for c in range(len(class_names)): # Prediksi\n",
    "            if r != c and cm[r, c] > 0:\n",
    "                errors.append({\n",
    "                    'Asli': class_names[r],\n",
    "                    'Diprediksi': class_names[c],\n",
    "                    'Jumlah': cm[r, c],\n",
    "                    'Persentase': f\"{(cm[r, c] / cm[r].sum())*100:.1f}% dari total {class_names[r]}\"\n",
    "                })\n",
    "    \n",
    "    # Tampilkan Top 3 Error\n",
    "    if errors:\n",
    "        errors_df = pd.DataFrame(errors).sort_values('Jumlah', ascending=False).head(3)\n",
    "        for _, row in errors_df.iterrows():\n",
    "            print(f\"   ‚ö†Ô∏è  Asli '{row['Asli']}' --> Salah deteksi jadi '{row['Diprediksi']}'\")\n",
    "            print(f\"       Jumlah: {row['Jumlah']} sampel ({row['Persentase']})\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Sempurna! Tidak ada kesalahan klasifikasi.\")\n",
    "    print()\n",
    "\n",
    "# 4. Grafik Perbandingan Recall per Kelas (Summary Akhir)\n",
    "print(\"üîπ Ringkasan Sensitivitas (Recall) per Kelas Serangan...\")\n",
    "recall_list = []\n",
    "for name, model in trained_models.items():\n",
    "    preds = model.predict(X_test_selected)\n",
    "    cm = confusion_matrix(y_test_final, preds)\n",
    "    recalls = np.diag(cm) / cm.sum(axis=1) # Recall per kelas\n",
    "    for idx, val in enumerate(recalls):\n",
    "        recall_list.append({'Metode': name, 'Kelas': class_names[idx], 'Recall': val})\n",
    "\n",
    "df_recall = pd.DataFrame(recall_list).pivot(index='Kelas', columns='Metode', values='Recall')\n",
    "ax = df_recall.plot(kind='bar', figsize=(10, 5), width=0.8, edgecolor='black')\n",
    "plt.title(\"Sensitivitas Deteksi per Jenis Serangan\", fontweight='bold')\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.axhline(0.8, color='red', linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('recall_summary_chart.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ CELL 10B SELESAI. Analisis tersimpan.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9D. Optimization Convergence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 12: Optimization Process Visualization (Convergence) - FIXED UNITS\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìà OPTIMIZATION PROCESS VISUALIZATION - TRACKING KONVERGENSI\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. VALIDASI DATA & KONVERSI UNIT\n",
    "# ================================================================================\n",
    "\n",
    "# Ambil ukuran validation set untuk konversi waktu\n",
    "try:\n",
    "    VAL_SIZE = len(X_val)\n",
    "    print(f\"‚ÑπÔ∏è  Ukuran Validation Set: {VAL_SIZE:,} sampel\")\n",
    "except NameError:\n",
    "    raise ValueError(\"‚ùå X_val tidak ditemukan. Jalankan Cell 3 terlebih dahulu!\")\n",
    "\n",
    "available_studies = {}\n",
    "if 'study_tpe' in locals() and study_tpe: available_studies['TPE'] = study_tpe\n",
    "if 'study_nsga' in locals() and study_nsga: available_studies['NSGA-II'] = study_nsga\n",
    "if 'study_random' in locals() and study_random: available_studies['Random'] = study_random\n",
    "\n",
    "if not available_studies:\n",
    "    raise ValueError(\"‚ùå Tidak ada study yang tersedia! Jalankan Cell 5-7 dulu.\")\n",
    "\n",
    "print(f\"‚úÖ Studies siap divisualisasi: {list(available_studies.keys())}\\n\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. PENGUMPULAN DATA TRIAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìã Mengumpulkan data history...\")\n",
    "\n",
    "all_trials_data = []\n",
    "\n",
    "for method_name, study in available_studies.items():\n",
    "    # Filter hanya trial yang sukses\n",
    "    successful_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    \n",
    "    # Identifikasi Pareto Trials (Solusi Terbaik)\n",
    "    pareto_ids = set([t.number for t in study.best_trials])\n",
    "\n",
    "    for trial in successful_trials:\n",
    "        # Values[0] = F1, Values[1] = Total Time (Seconds)\n",
    "        f1_score = trial.values[0]\n",
    "        raw_time = trial.values[1]\n",
    "        \n",
    "        # KONVERSI KE MIKRODETIK\n",
    "        latency_us = (raw_time / VAL_SIZE) * 1_000_000\n",
    "        \n",
    "        all_trials_data.append({\n",
    "            'Method': method_name,\n",
    "            'Trial_Number': trial.number,\n",
    "            'F1_Score': f1_score,\n",
    "            'Latency_us': latency_us,\n",
    "            'Is_Pareto': trial.number in pareto_ids\n",
    "        })\n",
    "\n",
    "trials_df = pd.DataFrame(all_trials_data)\n",
    "print(f\"‚úÖ Total data trials: {len(trials_df)}\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 3. VISUALISASI OPTIMIZATION HISTORY\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìä Membuat Plot Konvergensi...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Definisi Warna Konsisten\n",
    "colors = {'TPE': '#3498db', 'NSGA-II': '#e74c3c', 'Random': '#95a5a6'}\n",
    "\n",
    "# --- PLOT 1: F1-SCORE HISTORY ---\n",
    "ax1 = axes[0]\n",
    "for name in available_studies.keys():\n",
    "    subset = trials_df[trials_df['Method'] == name].sort_values('Trial_Number')\n",
    "    \n",
    "    # Plot Garis Tipis (Trajectory)\n",
    "    ax1.plot(subset['Trial_Number'], subset['F1_Score'], color=colors[name], \n",
    "             alpha=0.4, linewidth=1, label=f'{name} (History)')\n",
    "    \n",
    "    # Plot Titik Pareto (Bintang Besar)\n",
    "    pareto_subset = subset[subset['Is_Pareto']]\n",
    "    ax1.scatter(pareto_subset['Trial_Number'], pareto_subset['F1_Score'], \n",
    "                color=colors[name], s=150, marker='*', edgecolors='black', zorder=10,\n",
    "                label=f'{name} (Best/Pareto)')\n",
    "\n",
    "ax1.set_title('Riwayat Optimasi: F1-Score (Akurasi)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Macro F1-Score', fontsize=12)\n",
    "ax1.set_xlabel('Iterasi Trial', fontsize=12)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "ax1.legend(loc='lower right', ncol=3)\n",
    "\n",
    "# --- PLOT 2: LATENCY HISTORY (MIKRODETIK) ---\n",
    "ax2 = axes[1]\n",
    "for name in available_studies.keys():\n",
    "    subset = trials_df[trials_df['Method'] == name].sort_values('Trial_Number')\n",
    "    \n",
    "    # Plot Garis Tipis\n",
    "    ax2.plot(subset['Trial_Number'], subset['Latency_us'], color=colors[name], \n",
    "             alpha=0.4, linewidth=1)\n",
    "    \n",
    "    # Plot Titik Pareto\n",
    "    pareto_subset = subset[subset['Is_Pareto']]\n",
    "    ax2.scatter(pareto_subset['Trial_Number'], pareto_subset['Latency_us'], \n",
    "                color=colors[name], s=150, marker='*', edgecolors='black', zorder=10)\n",
    "\n",
    "ax2.set_title('Riwayat Optimasi: Latensi Inferensi (Kecepatan)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Latency (¬µs / sample)', fontsize=12)\n",
    "ax2.set_xlabel('Iterasi Trial', fontsize=12)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "# ax2.set_yscale('log') # Opsional: Aktifkan jika rentang waktu sangat jauh\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_history_final.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Grafik disimpan: optimization_history_final.png\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 4. ANALISIS PENINGKATAN (IMPROVEMENT STATS)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìà STATISTIK PENINGKATAN PERFORMA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "for name in available_studies.keys():\n",
    "    subset = trials_df[trials_df['Method'] == name].sort_values('Trial_Number')\n",
    "    \n",
    "    # Baseline (Trial Pertama) vs Best (Max F1 / Min Latency)\n",
    "    first_f1 = subset.iloc[0]['F1_Score']\n",
    "    best_f1 = subset['F1_Score'].max()\n",
    "    \n",
    "    first_lat = subset.iloc[0]['Latency_us']\n",
    "    best_lat = subset['Latency_us'].min()\n",
    "    \n",
    "    imp_f1 = ((best_f1 - first_f1) / first_f1) * 100\n",
    "    imp_lat = ((first_lat - best_lat) / first_lat) * 100\n",
    "    \n",
    "    stats_list.append({\n",
    "        'Metode': name,\n",
    "        'First F1': first_f1,\n",
    "        'Best F1': best_f1,\n",
    "        'Gain F1 (%)': imp_f1,\n",
    "        'First Lat (¬µs)': first_lat,\n",
    "        'Best Lat (¬µs)': best_lat,\n",
    "        'Gain Lat (%)': imp_lat\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "print(stats_df[['Metode', 'Gain F1 (%)', 'Gain Lat (%)', 'Best F1', 'Best Lat (¬µs)']].to_string(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "# Export Data untuk Lampiran\n",
    "trials_df.to_csv('history_trials_data.csv', index=False)\n",
    "stats_df.to_csv('improvement_stats.csv', index=False)\n",
    "print(\"\\n‚úÖ Data history & statistik diekspor ke CSV.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 11: Hyperparameter Importance Analysis (Surrogate RF) - VISUAL UPGRADE\n",
    "# ================================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üî¨ HYPERPARAMETER IMPORTANCE ANALYSIS (RANDOM FOREST SURROGATE)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. VALIDASI STUDY DARI CELL 5-7\n",
    "# ================================================================================\n",
    "\n",
    "studies = {}\n",
    "if 'study_tpe' in locals() and study_tpe: studies['TPE'] = study_tpe\n",
    "if 'study_nsga' in locals() and study_nsga: studies['NSGA-II'] = study_nsga\n",
    "if 'study_random' in locals() and study_random: studies['Random'] = study_random\n",
    "\n",
    "if not studies:\n",
    "    raise RuntimeError(\"‚ùå Tidak ada study tersedia! Jalankan optimasi terlebih dahulu.\")\n",
    "\n",
    "print(f\"‚úÖ Studies siap dianalisis: {list(studies.keys())}\\n\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. FUNGSI ANALISIS IMPORTANCE (CORE LOGIC)\n",
    "# ================================================================================\n",
    "\n",
    "def get_rf_importance(study, objective_name, objective_index):\n",
    "    \"\"\"\n",
    "    Menghitung importance menggunakan Random Forest Regressor.\n",
    "    \"\"\"\n",
    "    # Ambil trial yang sukses\n",
    "    trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    if len(trials) < 5:\n",
    "        print(f\"   ‚ö†Ô∏è Data trial terlalu sedikit untuk {objective_name} (<5). Skip.\")\n",
    "        return None\n",
    "\n",
    "    # Ekstrak parameter numerik & target value\n",
    "    data = []\n",
    "    for t in trials:\n",
    "        row = {k: v for k, v in t.params.items() if isinstance(v, (int, float))}\n",
    "        row['target'] = t.values[objective_index]\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Cek variansi target (jika semua F1 sama, tidak bisa dihitung importance-nya)\n",
    "    if df['target'].nunique() <= 1:\n",
    "        print(f\"   ‚ö†Ô∏è Target {objective_name} konstan (tidak ada variasi). Skip.\")\n",
    "        return None\n",
    "\n",
    "    X = df.drop(columns='target')\n",
    "    y = df['target']\n",
    "\n",
    "    # Latih Random Forest Surrogate\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Ambil Feature Importance\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    return importances.sort_values(ascending=False)\n",
    "\n",
    "# ================================================================================\n",
    "# 3. FUNGSI VISUALISASI (SEABORN STYLE)\n",
    "# ================================================================================\n",
    "\n",
    "def plot_importance(importance_series, title, filename, color_palette=\"viridis\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot Bar Chart\n",
    "    sns.barplot(\n",
    "        x=importance_series.values,\n",
    "        y=importance_series.index,\n",
    "        palette=color_palette,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.xlabel(\"Relative Importance (0.0 - 1.0)\", fontsize=12)\n",
    "    plt.ylabel(\"Hyperparameter\", fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Simpan\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"   ‚úÖ Grafik disimpan: {filename}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. EKSEKUSI ANALISIS\n",
    "# ================================================================================\n",
    "\n",
    "# Dictionary untuk menyimpan hasil CSV gabungan\n",
    "combined_importance = []\n",
    "\n",
    "for method_name, study in studies.items():\n",
    "    print(f\"üìå Menganalisis Study: {method_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # --- Analisis 1: F1-Score Importance ---\n",
    "    # Objective index 0 = F1 (Maximize)\n",
    "    imp_f1 = get_rf_importance(study, \"F1-Score\", 0)\n",
    "    \n",
    "    if imp_f1 is not None:\n",
    "        print(f\"   üîπ Top 3 Param untuk F1-Score:\")\n",
    "        for param, score in imp_f1.head(3).items():\n",
    "            print(f\"      - {param:20s}: {score:.4f}\")\n",
    "            \n",
    "        plot_importance(\n",
    "            imp_f1, \n",
    "            f\"Hyperparameter Importance terhadap F1-Score ({method_name})\",\n",
    "            f\"importance_f1_{method_name.lower()}.png\",\n",
    "            \"Blues_r\"\n",
    "        )\n",
    "        \n",
    "        # Simpan ke list untuk CSV\n",
    "        for p, s in imp_f1.items():\n",
    "            combined_importance.append({'Metode': method_name, 'Target': 'F1', 'Param': p, 'Importance': s})\n",
    "\n",
    "    # --- Analisis 2: Inference Time Importance ---\n",
    "    # Objective index 1 = Latency (Minimize)\n",
    "    imp_time = get_rf_importance(study, \"Inference Time\", 1)\n",
    "    \n",
    "    if imp_time is not None:\n",
    "        print(f\"   üîπ Top 3 Param untuk Latency:\")\n",
    "        for param, score in imp_time.head(3).items():\n",
    "            print(f\"      - {param:20s}: {score:.4f}\")\n",
    "            \n",
    "        plot_importance(\n",
    "            imp_time, \n",
    "            f\"Hyperparameter Importance terhadap Latency ({method_name})\",\n",
    "            f\"importance_time_{method_name.lower()}.png\",\n",
    "            \"Reds_r\"\n",
    "        )\n",
    "\n",
    "        for p, s in imp_time.items():\n",
    "            combined_importance.append({'Metode': method_name, 'Target': 'Time', 'Param': p, 'Importance': s})\n",
    "            \n",
    "    print()\n",
    "\n",
    "# ================================================================================\n",
    "# 5. EXPORT CSV REKAP\n",
    "# ================================================================================\n",
    "\n",
    "if combined_importance:\n",
    "    df_imp = pd.DataFrame(combined_importance)\n",
    "    df_imp.to_csv(\"hyperparameter_importance_summary.csv\", index=False)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üíæ File rekap disimpan: hyperparameter_importance_summary.csv\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tidak ada data importance yang dihasilkan.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 13: Feature Importance Analysis (XGBoost) - FIXED\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üå≥ FEATURE IMPORTANCE ANALYSIS - XGBOOST (GAIN)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. VALIDASI KETERSEDIAAN MODEL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üîç Validasi ketersediaan models dari Cell 8...\")\n",
    "\n",
    "# PENTING: Gunakan 'trained_models' dari Cell 8\n",
    "if 'trained_models' not in locals() or len(trained_models) == 0:\n",
    "    raise ValueError(\"‚ùå 'trained_models' tidak ditemukan! Jalankan Cell 8 terlebih dahulu.\")\n",
    "\n",
    "print(f\"‚úÖ Models tersedia: {list(trained_models.keys())}\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 2. EKSTRAK FEATURE IMPORTANCE (GAIN)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìä Mengekstrak Feature Importance (Gain)...\")\n",
    "feature_importance_data = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\nüìå Metode: {name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Ambil booster dari model\n",
    "    booster = model.get_booster()\n",
    "    \n",
    "    # Ambil importance type 'gain' (paling relevan untuk akurasi)\n",
    "    importance_dict = booster.get_score(importance_type='gain')\n",
    "    \n",
    "    # Mapping agar jika ada fitur yang tidak terpilih tetap muncul dengan nilai 0\n",
    "    all_features = booster.feature_names\n",
    "    full_importance = {f: importance_dict.get(f, 0.0) for f in all_features}\n",
    "    \n",
    "    # Simpan\n",
    "    feature_importance_data[name] = full_importance\n",
    "    \n",
    "    # Tampilkan Top 5\n",
    "    sorted_imp = sorted(full_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"   Top 5 Fitur Paling Berpengaruh:\")\n",
    "    for f_name, f_val in sorted_imp[:5]:\n",
    "        print(f\"      - {f_name:20s}: {f_val:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. VISUALISASI 1: TOP 15 FEATURES PER MODEL (BAR CHART)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat Bar Chart Top 15 Features...\")\n",
    "\n",
    "n_methods = len(feature_importance_data)\n",
    "fig, axes = plt.subplots(1, n_methods, figsize=(6 * n_methods, 8))\n",
    "if n_methods == 1: axes = [axes]\n",
    "\n",
    "for ax, (name, imp_dict) in zip(axes, feature_importance_data.items()):\n",
    "    # Ambil Top 15\n",
    "    top_feats = sorted(imp_dict.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    feats = [x[0] for x in top_feats]\n",
    "    scores = [x[1] for x in top_feats]\n",
    "    \n",
    "    # Plot Horizontal Bar\n",
    "    ax.barh(feats, scores, color='#2ecc71', edgecolor='black')\n",
    "    ax.invert_yaxis() # Fitur terpenting di atas\n",
    "    ax.set_title(f\"Feature Importance ({name})\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(\"Average Gain\", fontsize=10)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_bar_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar disimpan: feature_importance_bar_comparison.png\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. VISUALISASI 2: HEATMAP COMPARISON (TOP 20 GLOBAL)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat Heatmap Komparasi...\")\n",
    "\n",
    "# 1. Cari Top 20 Fitur Global (Rata-rata dari semua model)\n",
    "global_scores = {}\n",
    "for imp in feature_importance_data.values():\n",
    "    for f, v in imp.items():\n",
    "        global_scores[f] = global_scores.get(f, 0) + v\n",
    "\n",
    "# Ambil Top 20 Global\n",
    "top_20_global = [x[0] for x in sorted(global_scores.items(), key=lambda x: x[1], reverse=True)[:20]]\n",
    "\n",
    "# 2. Buat DataFrame untuk Heatmap\n",
    "heatmap_data = []\n",
    "for name in feature_importance_data.keys():\n",
    "    row = [feature_importance_data[name].get(f, 0) for f in top_20_global]\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "df_heatmap = pd.DataFrame(heatmap_data, columns=top_20_global, index=feature_importance_data.keys())\n",
    "\n",
    "# 3. Normalisasi MinMax per Baris (agar warnanya adil antar metode)\n",
    "#    (Opsional: bisa dimatikan jika ingin lihat nilai absolut)\n",
    "df_heatmap_norm = df_heatmap.div(df_heatmap.max(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.heatmap(\n",
    "    df_heatmap_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    annot=False, # Annotasi angka dimatikan agar bersih\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Relative Importance (Normalized)'}\n",
    ")\n",
    "plt.title(\"Perbandingan Kepentingan Fitur Antar Metode (Top 20 Global)\", fontsize=14, fontweight='bold', pad=15)\n",
    "plt.xlabel(\"Fitur Jaringan\", fontsize=12)\n",
    "plt.ylabel(\"Metode Optimasi\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_heatmap_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar disimpan: feature_importance_heatmap_comparison.png\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. EXPORT CSV\n",
    "# ================================================================================\n",
    "print(\"\\nüíæ Export Data Importance...\")\n",
    "\n",
    "# Gabungkan jadi satu CSV\n",
    "df_export = pd.DataFrame(feature_importance_data)\n",
    "df_export.index.name = 'Feature'\n",
    "df_export.to_csv(\"feature_importance_all_methods.csv\")\n",
    "print(\"‚úÖ File CSV disimpan: feature_importance_all_methods.csv\")\n",
    "print(\"\\n‚úÖ CELL 13 SELESAI.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Performance Metrics per Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 14: Detailed Performance Metrics per Class - FIXED\n",
    "# ================================================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä DETAILED PERFORMANCE METRICS PER CLASS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. VALIDASI DATA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üîç Validasi ketersediaan data...\")\n",
    "\n",
    "# Gunakan 'trained_models' dari Cell 8\n",
    "if 'trained_models' not in locals() or len(trained_models) == 0:\n",
    "    raise ValueError(\"‚ùå 'trained_models' tidak ditemukan! Jalankan Cell 8 terlebih dahulu.\")\n",
    "\n",
    "if 'y_test_final' not in locals():\n",
    "    raise ValueError(\"‚ùå 'y_test_final' tidak ditemukan! Jalankan Cell 3 terlebih dahulu.\")\n",
    "\n",
    "print(f\"‚úÖ Models tersedia: {list(trained_models.keys())}\")\n",
    "print(f\"‚úÖ Test samples: {len(y_test_final):,}\")\n",
    "print()\n",
    "\n",
    "# Mapping nama kelas yang konsisten\n",
    "label_map = {0: 'Normal', 1: 'DoS', 2: 'Probe', 3: 'Malware'}\n",
    "class_names = [label_map.get(i, str(i)) for i in range(len(label_map))]\n",
    "\n",
    "# ================================================================================\n",
    "# 2. HITUNG METRICS PER CLASS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìä Menghitung Precision, Recall, F1 per kelas...\")\n",
    "all_metrics = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Prediksi ulang (cepat)\n",
    "    preds = model.predict(X_test_selected)\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test_final, \n",
    "        preds, \n",
    "        average=None, \n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    all_metrics[name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "    print(f\"   ‚úÖ Metrics {name} calculated.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. VISUALISASI 1: GROUPED BAR CHART (METRICS PER METHOD)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat Grouped Bar Chart...\")\n",
    "\n",
    "n_methods = len(trained_models)\n",
    "fig, axes = plt.subplots(n_methods, 1, figsize=(12, 6 * n_methods))\n",
    "if n_methods == 1: axes = [axes]\n",
    "\n",
    "for ax, (name, metrics) in zip(axes, all_metrics.items()):\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Plot 3 Bar berdampingan: Precision, Recall, F1\n",
    "    ax.bar(x - width, metrics['precision'], width, label='Precision', color='#3498db', edgecolor='black')\n",
    "    ax.bar(x, metrics['recall'], width, label='Recall', color='#e74c3c', edgecolor='black')\n",
    "    ax.bar(x + width, metrics['f1'], width, label='F1-Score', color='#2ecc71', edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f\"Detailed Metrics: {name}\", fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(\"Score (0.0 - 1.0)\", fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=0, fontsize=11)\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    ax.legend(loc='upper center', ncol=3, fontsize=10)\n",
    "    \n",
    "    # Anotasi Support (Jumlah Sampel) di bawah sumbu X\n",
    "    for i, n in enumerate(metrics['support']):\n",
    "        ax.text(i, -0.15, f\"(n={n})\", ha='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_grouped_bar.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar disimpan: metrics_grouped_bar.png\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. VISUALISASI 2: HEATMAP F1-SCORE COMPARISON\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat Heatmap F1-Score Comparison...\")\n",
    "\n",
    "# Buat DataFrame F1 Score\n",
    "f1_data = []\n",
    "for name, metrics in all_metrics.items():\n",
    "    f1_data.append(metrics['f1'])\n",
    "\n",
    "df_f1 = pd.DataFrame(f1_data, columns=class_names, index=all_metrics.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(\n",
    "    df_f1, \n",
    "    annot=True, \n",
    "    fmt=\".3f\", \n",
    "    cmap=\"RdYlGn\", \n",
    "    vmin=0.0, \n",
    "    vmax=1.0, \n",
    "    linewidths=1, \n",
    "    linecolor='black',\n",
    "    cbar_kws={'label': 'F1-Score'}\n",
    ")\n",
    "plt.title(\"Perbandingan F1-Score Antar Metode (Per Kelas)\", fontsize=14, fontweight='bold', pad=15)\n",
    "plt.xlabel(\"Kelas Serangan\", fontsize=12)\n",
    "plt.ylabel(\"Metode Optimasi\", fontsize=12)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_f1_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar disimpan: metrics_f1_heatmap.png\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. VISUALISASI 3: PRECISION-RECALL SCATTER PLOT\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Membuat Precision-Recall Scatter Plot...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = {'TPE': '#3498db', 'NSGA-II': '#e74c3c', 'Random': '#95a5a6'}\n",
    "markers = ['o', 's', '^', 'D'] # Normal, DoS, Probe, Malware\n",
    "\n",
    "# Dummy plot for legend\n",
    "for name, color in colors.items():\n",
    "    if name in all_metrics:\n",
    "        ax.scatter([], [], c=color, label=f\"Metode: {name}\", s=100)\n",
    "for i, cls in enumerate(class_names):\n",
    "    ax.scatter([], [], c='black', marker=markers[i], label=f\"Kelas: {cls}\", s=100)\n",
    "\n",
    "# Plot Data Asli\n",
    "for name, metrics in all_metrics.items():\n",
    "    for i, cls in enumerate(class_names):\n",
    "        ax.scatter(\n",
    "            metrics['recall'][i], \n",
    "            metrics['precision'][i], \n",
    "            c=colors[name], \n",
    "            marker=markers[i], \n",
    "            s=150, \n",
    "            alpha=0.8, \n",
    "            edgecolors='black'\n",
    "        )\n",
    "        \n",
    "        # Opsional: Tulis nama kelas kecil di dekat titik\n",
    "        # ax.text(metrics['recall'][i]+0.01, metrics['precision'][i]+0.01, cls, fontsize=8)\n",
    "\n",
    "ax.set_title(\"Precision vs Recall Space\", fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel(\"Recall (Sensitivitas)\", fontsize=12)\n",
    "ax.set_ylabel(\"Precision (Presisi)\", fontsize=12)\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Legenda\")\n",
    "\n",
    "# Garis Ideal (Perfect Score)\n",
    "ax.plot([1], [1], 'r*', markersize=15, label='Ideal Point')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_pr_scatter.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Gambar disimpan: metrics_pr_scatter.png\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6. EXPORT CSV PER KELAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Export CSV Metrics...\")\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "    df_export = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1'],\n",
    "        'Support': metrics['support']\n",
    "    })\n",
    "    filename = f\"metrics_detailed_{name.lower()}.csv\"\n",
    "    df_export.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Disimpan: {filename}\")\n",
    "\n",
    "print(\"\\n‚úÖ CELL 14 SELESAI.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Statistical Validation (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 16: Statistical Validation - Cross-Validation (F1 & Time) - ROBUST\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä STATISTICAL VALIDATION: F1-SCORE vs INFERENCE TIME\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Tujuan: Menguji signifikansi perbedaan Kualitas Deteksi & Efisiensi Waktu.\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. VALIDASI VARIABLE (AUTO-DETECT)\n",
    "# ================================================================================\n",
    "\n",
    "if 'trained_models' in locals():\n",
    "    target_models = trained_models\n",
    "    print(\"‚úÖ Menggunakan model dari: 'trained_models'\")\n",
    "elif 'global_models' in locals():\n",
    "    target_models = global_models\n",
    "    print(\"‚úÖ Menggunakan model dari: 'global_models'\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Tidak ada model ditemukan! Jalankan Cell 8 terlebih dahulu.\")\n",
    "\n",
    "if 'X_train' not in locals() or 'y_train' not in locals():\n",
    "    raise ValueError(\"‚ùå Training data tidak ditemukan!\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. KONFIGURASI\n",
    "# ================================================================================\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_SIZE = 0.20  # 20% data untuk efisiensi waktu CV\n",
    "\n",
    "print(\"‚öôÔ∏è Konfigurasi Cross-Validation:\")\n",
    "print(f\"   - K-Fold:          {N_FOLDS}\")\n",
    "print(f\"   - Sample Size:     {SAMPLE_SIZE * 100}% dari Data Latih\")\n",
    "print(f\"   - Metrics:         Macro F1-Score & Inference Time\")\n",
    "\n",
    "# Sampling Data untuk CV\n",
    "X_cv, _, y_cv, _, w_cv, _ = train_test_split(\n",
    "    X_train, y_train, sample_weights_train,\n",
    "    train_size=SAMPLE_SIZE, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"   - CV Samples:      {len(X_cv):,} baris\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 3. RUNNING CROSS-VALIDATION\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üîÑ Menjalankan Cross-Validation...\")\n",
    "cv_results = {}\n",
    "\n",
    "for name, obj in target_models.items():\n",
    "    print(f\"\\nüìå Metode: {name}\")\n",
    "    \n",
    "    # Handle struktur dictionary lama vs baru\n",
    "    base_model = obj['model'] if isinstance(obj, dict) and 'model' in obj else obj\n",
    "    \n",
    "    # Re-instantiate model baru dengan parameter sama\n",
    "    params = base_model.get_params()\n",
    "    # Pastikan kompatibilitas GPU\n",
    "    if params.get('device') == 'cuda':\n",
    "        params['tree_method'] = 'hist'\n",
    "        \n",
    "    cv_model = xgb.XGBClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    scores_f1 = []\n",
    "    scores_time = [] \n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_cv, y_cv), start=1):\n",
    "        X_tr, X_va = X_cv.iloc[tr_idx], X_cv.iloc[va_idx]\n",
    "        y_tr, y_va = y_cv[tr_idx], y_cv[va_idx]\n",
    "        w_tr = w_cv[tr_idx]\n",
    "\n",
    "        # Train\n",
    "        cv_model.fit(X_tr, y_tr, sample_weight=w_tr, verbose=False)\n",
    "\n",
    "        # Inference Time Measurement (Critical)\n",
    "        start_time = time.perf_counter()\n",
    "        y_pred = cv_model.predict(X_va)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Simpan dalam satuan DETIK per Fold\n",
    "        inf_duration = end_time - start_time\n",
    "        \n",
    "        f1 = f1_score(y_va, y_pred, average=\"macro\")\n",
    "        scores_f1.append(f1)\n",
    "        scores_time.append(inf_duration)\n",
    "        \n",
    "        print(f\"   Fold {fold} | F1: {f1:.4f} | Time: {inf_duration:.4f}s\")\n",
    "\n",
    "    cv_results[name] = {\n",
    "        \"f1_scores\": scores_f1, \"time_scores\": scores_time,\n",
    "        \"f1_mean\": np.mean(scores_f1), \"time_mean\": np.mean(scores_time)\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# 4. VISUALISASI BOXPLOT\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Visualisasi Boxplot Distribusi...\")\n",
    "valid_results = {k: v for k, v in cv_results.items() if len(v[\"f1_scores\"]) > 0}\n",
    "\n",
    "if valid_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    labels = list(valid_results.keys())\n",
    "    \n",
    "    # Plot F1\n",
    "    data_f1 = [v[\"f1_scores\"] for v in valid_results.values()]\n",
    "    sns.boxplot(data=data_f1, ax=axes[0], palette=\"Blues\")\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_title(\"Stabilitas Akurasi (Macro F1)\", fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"F1 Score\")\n",
    "    \n",
    "    # Plot Time\n",
    "    data_time = [v[\"time_scores\"] for v in valid_results.values()]\n",
    "    sns.boxplot(data=data_time, ax=axes[1], palette=\"Reds\")\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_title(\"Efisiensi Waktu (Lower is Better)\", fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Inference Time (Seconds/Fold)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cv_stats_boxplot.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# ================================================================================\n",
    "# 5. UJI HIPOTESIS (KRUSKAL-WALLIS)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä UJI STATISTIK (Kruskal-Wallis Significance Test)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric, key, goal in [(\"F1-Score\", \"f1_scores\", \"max\"), (\"Time\", \"time_scores\", \"min\")]:\n",
    "    print(f\"üîπ Testing {metric} ({'Higher' if goal=='max' else 'Lower'} is Better):\")\n",
    "    \n",
    "    samples = [valid_results[m][key] for m in valid_results]\n",
    "    stat, p = kruskal(*samples)\n",
    "    \n",
    "    print(f\"   H-stat: {stat:.4f}, p-value: {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"   ‚úÖ SIGNIFIKAN: Ada perbedaan performa yang nyata antar metode.\")\n",
    "        # Simple Winner Check\n",
    "        means = {m: np.mean(valid_results[m][key]) for m in valid_results}\n",
    "        winner = max(means, key=means.get) if goal == 'max' else min(means, key=means.get)\n",
    "        print(f\"   üèÜ Pemenang Statistik: {winner}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è TIDAK SIGNIFIKAN: Performa metode dianggap setara.\")\n",
    "    print()\n",
    "\n",
    "# Export Summary\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"Method\": m, \n",
    "    \"Mean_F1\": r[\"f1_mean\"], \n",
    "    \"Mean_Time\": r[\"time_mean\"]\n",
    "} for m, r in cv_results.items()])\n",
    "summary_df.to_csv(\"cv_statistical_summary.csv\", index=False)\n",
    "print(\"‚úÖ Summary disimpan: cv_statistical_summary.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell 18: Final Summary & Deployment Recommendations (ROBUST)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ FINAL SUMMARY & DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 1. STRUKTUR DATA UTAMA\n",
    "# ================================================================================\n",
    "\n",
    "final_summary = {\n",
    "    \"metadata\": {},\n",
    "    \"best_model_performance\": {},\n",
    "    \"optimization_stats\": {},\n",
    "    \"feature_importance_top10\": [],\n",
    "    \"cross_validation_stats\": []\n",
    "}\n",
    "\n",
    "# ================================================================================\n",
    "# 2. METADATA DATASET\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìå Section 1: Dataset & Configuration\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Cek ketersediaan variabel dasar\n",
    "n_train = len(X_train) if 'X_train' in locals() else 0\n",
    "n_test = len(X_test_selected) if 'X_test_selected' in locals() else 0\n",
    "n_feats = X_train.shape[1] if 'X_train' in locals() else 0\n",
    "n_classes = len(class_names) if 'class_names' in locals() else 0\n",
    "\n",
    "final_summary[\"metadata\"] = {\n",
    "    \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset_name\": \"NF-UNSW-NB15-v3\",\n",
    "    \"total_samples\": n_train + n_test,\n",
    "    \"train_samples\": n_train,\n",
    "    \"test_samples\": n_test,\n",
    "    \"n_features\": n_feats,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"class_labels\": class_names if 'class_names' in locals() else []\n",
    "}\n",
    "\n",
    "for k, v in final_summary[\"metadata\"].items():\n",
    "    print(f\"   {k.replace('_', ' ').title():20s}: {v}\")\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 3. PERFORMA MODEL TERBAIK (EVALUASI TEST SET)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìå Section 2: Best Model Performance (Test Set)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'df_results' in locals() and not df_results.empty:\n",
    "    # Ambil baris terbaik berdasarkan F1-Macro\n",
    "    best_row = df_results.loc[df_results['F1_Macro'].idxmax()]\n",
    "    \n",
    "    final_summary[\"best_model_performance\"] = {\n",
    "        \"method\": best_row.name, # Index adalah nama metode\n",
    "        \"f1_macro\": best_row['F1_Macro'],\n",
    "        \"accuracy\": best_row['Accuracy'],\n",
    "        \"latency_us\": best_row['Latency (¬µs)'] if 'Latency (¬µs)' in best_row else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"   üèÜ BEST METHOD      : {best_row.name}\")\n",
    "    print(f\"      - F1 Macro       : {best_row['F1_Macro']:.4f}\")\n",
    "    print(f\"      - Accuracy       : {best_row['Accuracy']:.4f}\")\n",
    "    print(f\"      - Latency        : {best_row['Latency (¬µs)']:.2f} ¬µs\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Data evaluasi (df_results) tidak ditemukan.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 4. STATISTIK CROSS-VALIDATION (F1 & TIME)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìå Section 3: Cross-Validation Stats (Stability)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'cv_results' in locals() and cv_results:\n",
    "    for method, res in cv_results.items():\n",
    "        # Validasi struktur data\n",
    "        if \"f1_mean\" in res and \"time_mean\" in res:\n",
    "            mean_f1 = res[\"f1_mean\"]\n",
    "            std_f1 = res.get(\"f1_std\", 0)\n",
    "            mean_time = res[\"time_mean\"]\n",
    "            std_time = res.get(\"time_std\", 0)\n",
    "            \n",
    "            stats = {\n",
    "                \"method\": method,\n",
    "                \"mean_f1\": mean_f1,\n",
    "                \"std_f1\": std_f1,\n",
    "                \"mean_time_sec\": mean_time,\n",
    "                \"std_time_sec\": std_time\n",
    "            }\n",
    "            final_summary[\"cross_validation_stats\"].append(stats)\n",
    "            \n",
    "            print(f\"   üîπ {method}:\")\n",
    "            print(f\"      - F1 Score       : {mean_f1:.4f} (¬±{std_f1:.4f})\")\n",
    "            print(f\"      - Inference Time : {mean_time:.6f}s (¬±{std_time:.6f}s)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Data CV (cv_results) tidak ditemukan.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 5. FEATURE IMPORTANCE (TOP 10)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìå Section 4: Top 10 Feature Importance\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'feature_importance_data' in locals() and feature_importance_data:\n",
    "    # Agregasi skor dari semua model untuk dapat Global Top 10\n",
    "    global_scores = {}\n",
    "    for imp in feature_importance_data.values():\n",
    "        for f, v in imp.items():\n",
    "            global_scores[f] = global_scores.get(f, 0) + v\n",
    "            \n",
    "    # Sort\n",
    "    top_10 = sorted(global_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    for rank, (feat, score) in enumerate(top_10, 1):\n",
    "        final_summary[\"feature_importance_top10\"].append({\"feature\": feat, \"score\": score})\n",
    "        print(f\"   {rank:2d}. {feat:20s}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Data feature importance tidak ditemukan.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ================================================================================\n",
    "# 6. REKOMENDASI DEPLOYMENT\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üìå Section 5: Deployment Recommendations\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_method = final_summary[\"best_model_performance\"].get(\"method\", \"Unknown\")\n",
    "latency = final_summary[\"best_model_performance\"].get(\"latency_us\", 999)\n",
    "\n",
    "rec_text = f\"\"\"\n",
    "1. **Model Selection**: Gunakan model hasil optimasi **{best_method}** karena memiliki keseimbangan terbaik antara akurasi dan kecepatan.\n",
    "2. **Infrastructure**: \n",
    "   - Latensi model tercatat **{latency:.2f} ¬µs** per sampel. \n",
    "   - Ini sangat memadai untuk dijalankan pada perangkat Edge (IoT Gateway) atau Cloud Instance standar (CPU-only support is viable).\n",
    "3. **Monitoring**: \n",
    "   - Fitur terpenting adalah '{final_summary[\"feature_importance_top10\"][0][\"feature\"]}' dan '{final_summary[\"feature_importance_top10\"][1][\"feature\"]}'.\n",
    "   - Pastikan logging difokuskan pada fitur-fitur ini untuk deteksi dini anomali.\n",
    "4. **Maintenance**: \n",
    "   - Lakukan retraining berkala jika pola trafik berubah drastis (Concept Drift), terutama jika distribusi fitur '{final_summary[\"feature_importance_top10\"][0][\"feature\"]}' bergeser.\n",
    "\"\"\"\n",
    "\n",
    "print(rec_text)\n",
    "\n",
    "# ================================================================================\n",
    "# 7. EXPORT JSON SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "filename_json = \"final_project_summary.json\"\n",
    "try:\n",
    "    with open(filename_json, \"w\") as f:\n",
    "        json.dump(final_summary, f, indent=4, default=str) # default=str handle numpy types\n",
    "    print(f\"‚úÖ Summary lengkap disimpan ke: {filename_json}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gagal menyimpan JSON: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ SELURUH RANGKAIAN EKSPERIMEN SELESAI!\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell Final: Export ALL Pareto Models (Updated with Scenario Logic)\n",
    "# ================================================================================\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "import joblib\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üì¶ FINAL EXPORT: PARETO FRONTS WITH SCENARIO TESTING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Tujuan: Menyimpan model & Membuat Dashboard dengan 2 Skenario Pengujian.\")\n",
    "print()\n",
    "\n",
    "# 1. Buat struktur direktori\n",
    "base_dir = \"nids_dashboard_complete\"\n",
    "models_dir = os.path.join(base_dir, \"models\")\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "os.makedirs(models_dir)\n",
    "\n",
    "print(f\"üìÇ Direktori dibuat: {base_dir}/\")\n",
    "print(f\"üìÇ Sub-direktori model: {models_dir}/\")\n",
    "\n",
    "# List untuk menampung metadata semua model yang disimpan\n",
    "catalog_data = []\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNGSI HELPER: RETRAIN & SAVE\n",
    "# ==============================================================================\n",
    "def save_trial_model(trial, method_name, label_suffix):\n",
    "    # 1. Ambil Parameter\n",
    "    params = trial.params.copy()\n",
    "    params.update({\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(le_target.classes_),\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42\n",
    "    })\n",
    "    \n",
    "    # 2. Retrain Model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "    \n",
    "    # 3. Hitung Metadata\n",
    "    f1 = trial.values[0]\n",
    "    \n",
    "    if isinstance(trial.values, list) and len(trial.values) > 1:\n",
    "        lat_sec = trial.values[1]\n",
    "    else:\n",
    "        lat_sec = trial.user_attrs.get('total_time_sec', 0.05)\n",
    "\n",
    "    lat_us = (lat_sec / len(X_val)) * 1_000_000\n",
    "    \n",
    "    # 4. Simpan File\n",
    "    safe_suffix = label_suffix.replace(\" \", \"_\").replace(\"#\", \"\").lower()\n",
    "    filename = f\"model_{method_name.lower()}_{safe_suffix}.json\"\n",
    "    save_path = os.path.join(models_dir, filename)\n",
    "    model.save_model(save_path)\n",
    "    \n",
    "    # 5. Catat ke Katalog\n",
    "    ui_label = f\"[{method_name}] {label_suffix} | F1: {f1:.4f} | {lat_us:.1f} ¬µs\"\n",
    "    \n",
    "    catalog_data.append({\n",
    "        \"method\": method_name,\n",
    "        \"filename\": filename,\n",
    "        \"label\": ui_label,\n",
    "        \"f1_score\": f1,\n",
    "        \"latency_us\": lat_us,\n",
    "        \"n_estimators\": params['n_estimators'],\n",
    "        \"max_depth\": params['max_depth']\n",
    "    })\n",
    "    print(f\"   ‚úÖ Saved: {ui_label}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNGSI EKSPOR GENERIK\n",
    "# ==============================================================================\n",
    "def export_pareto_front(study, method_name):\n",
    "    print(f\"\\nüîÑ Memproses Pareto Front untuk: {method_name}...\")\n",
    "    if study is None or not study.best_trials:\n",
    "        print(f\"   ‚ö†Ô∏è Studi {method_name} tidak ditemukan/kosong.\")\n",
    "        return\n",
    "\n",
    "    pareto_trials = study.best_trials\n",
    "    pareto_trials.sort(key=lambda x: x.values[0], reverse=True)\n",
    "\n",
    "    for i, trial in enumerate(pareto_trials):\n",
    "        suffix = f\"Pareto #{i+1}\"\n",
    "        save_trial_model(trial, method_name, suffix)\n",
    "\n",
    "# ==============================================================================\n",
    "# EKSEKUSI EXPORT\n",
    "# ==============================================================================\n",
    "export_pareto_front(locals().get('study_tpe'), \"TPE\")\n",
    "export_pareto_front(locals().get('study_random'), \"Random\")\n",
    "export_pareto_front(locals().get('study_nsga'), \"NSGA-II\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SIMPAN ARTEFAK\n",
    "# ==============================================================================\n",
    "print(\"\\nüì¶ Menyimpan Metadata & Artefak...\")\n",
    "if catalog_data:\n",
    "    df_cat = pd.DataFrame(catalog_data)\n",
    "    df_cat.to_csv(os.path.join(base_dir, \"models_catalog.csv\"), index=False)\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Tidak ada model yang disimpan!\")\n",
    "\n",
    "scaler_var = locals().get('scaler') or locals().get('std_scaler')\n",
    "if scaler_var: joblib.dump(scaler_var, os.path.join(base_dir, \"scaler.pkl\"))\n",
    "if 'le_target' in locals(): joblib.dump(le_target, os.path.join(base_dir, \"label_encoder.pkl\"))\n",
    "\n",
    "if 'X_test_selected' in locals():\n",
    "    # Simpan sampel simulasi\n",
    "    sim_df = X_test_selected.iloc[:2000].copy()\n",
    "    sim_df['Label_True'] = y_test_final[:2000]\n",
    "    sim_df.to_csv(os.path.join(base_dir, \"simulation_data.csv\"), index=False)\n",
    "    print(\"   ‚úÖ simulation_data.csv disimpan.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================================\n",
    "# Cell Final Backup: Archive All Thesis Results (PNG, CSV, JSON, Models)\n",
    "# ================================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üì¶ FINAL BACKUP: MENGARSIPKAN SELURUH HASIL SKRIPSI\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# 1. Konfigurasi Nama File Backup\n",
    "# Tambahkan timestamp agar file tidak tertimpa jika di-run berkali-kali\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "backup_filename = f\"ARSIP_LENGKAP_SKRIPSI_{timestamp}\"\n",
    "backup_dir = \"temp_backup_folder\"\n",
    "\n",
    "# 2. Buat Folder Sementara\n",
    "if os.path.exists(backup_dir):\n",
    "    shutil.rmtree(backup_dir)\n",
    "os.makedirs(backup_dir)\n",
    "\n",
    "# 3. Definisi Pola File yang Akan Diambil\n",
    "# Kita hanya mengambil file hasil, bukan file sistem/config\n",
    "file_patterns = [\n",
    "    \"*.png\",       # Semua Grafik Visualisasi\n",
    "    \"*.csv\",       # Semua Tabel Hasil (Metrics, Feature Imp, Logs)\n",
    "    \"*.json\",      # Summary & Model JSON\n",
    "    \"*.pkl\",       # Scaler & Label Encoder\n",
    "    \"*.html\",      # Plotly Interactive Plots\n",
    "    \"app.py\",      # Source code aplikasi streamlit\n",
    "    \"requirements.txt\" # Jika ada\n",
    "]\n",
    "\n",
    "print(f\"üìÇ Mengumpulkan file ke folder: {backup_dir}/\")\n",
    "\n",
    "count = 0\n",
    "for pattern in file_patterns:\n",
    "    # Cari file sesuai pola\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    for f in files:\n",
    "        # Hindari meng-copy file zip backup itu sendiri (looping)\n",
    "        if f.endswith(\".zip\"):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            shutil.copy2(f, backup_dir)\n",
    "            count += 1\n",
    "            print(f\"   ‚úÖ Copied: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Gagal copy {f}: {e}\")\n",
    "\n",
    "# 4. Tambahkan Folder Model (Jika ada folder hasil export sebelumnya)\n",
    "# Kita cari folder hasil export dari cell sebelumnya (Complete / Dashboard)\n",
    "possible_model_dirs = [\"nids_dashboard_complete\", \"nids_dashboard_package\", \"nids_model_export\"]\n",
    "model_dir_found = False\n",
    "\n",
    "for d in possible_model_dirs:\n",
    "    if os.path.exists(d):\n",
    "        target_path = os.path.join(backup_dir, d)\n",
    "        shutil.copytree(d, target_path)\n",
    "        print(f\"   üì¶ Menyertakan folder model: {d}/\")\n",
    "        model_dir_found = True\n",
    "        break # Ambil satu saja yang paling update\n",
    "\n",
    "if not model_dir_found:\n",
    "    print(\"   ‚ÑπÔ∏è  Info: Tidak ditemukan folder model paket terpisah (hanya file root).\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"üìä Total file diamankan: {count}\")\n",
    "\n",
    "# 5. Membuat File ZIP Akhir\n",
    "print(f\"‚è≥ Sedang memadatkan (Zipping)...\")\n",
    "shutil.make_archive(backup_filename, 'zip', backup_dir)\n",
    "\n",
    "# 6. Bersihkan Folder Sementara\n",
    "shutil.rmtree(backup_dir)\n",
    "\n",
    "# 7. Cek Ukuran File\n",
    "zip_size_mb = os.path.getsize(f\"{backup_filename}.zip\") / (1024 * 1024)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üéâ SUKSES! FILE BACKUP SIAP DOWNLOAD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÇ Nama File : {backup_filename}.zip\")\n",
    "print(f\"üíæ Ukuran    : {zip_size_mb:.2f} MB\")\n",
    "print()\n",
    "print(\"üëâ INSTRUKSI DOWNLOAD:\")\n",
    "print(\"1. Lihat panel 'Files' di sebelah KANAN (Kaggle) atau KIRI (Colab).\")\n",
    "print(f\"2. Cari file '{backup_filename}.zip'.\")\n",
    "print(\"3. Klik menu (titik tiga) -> Download.\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}